{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### week3\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "- regression examples\n",
    "- Gaussian (Normal) distribution\n",
    "- likelihood and maximum likelihood estimate\n",
    "- coordinate descent\n",
    "- multivariate regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import notebook as nb\n",
    "import mlai\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review and Preview\n",
    "\n",
    "- Last week we minimised a sum of squares objective function (ie, least squares) by (steepest) gradient descent and stochastic gradient descent algorithms\n",
    "\n",
    "- This week we explore least squares for univariate and multivariate regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression Analysis\n",
    "\n",
    "- statistical process for estimating the relationships among variables\n",
    "- predict a real value, $y_i$, given some input $x_i$\n",
    "\n",
    "Wikipedia: [Regression analysis](https://en.wikipedia.org/wiki/Regression_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m\n",
    "\n",
    "Wikimedia commons: [London 2012 Olympic 100m final start](http://bit.ly/191adDC)\n",
    "\n",
    "Using regression we want to predict a winning time for the next Olympic game.\n",
    "\n",
    "- graph showing gold medal times since 1896 (unit: seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18853491f60>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAGnCAYAAADMoeKHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18853491e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pods.datasets.olympic_100m_men()\n",
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(data['X'], data['Y'], 'r.', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon\n",
    "\n",
    "Wikimedia commons: [Stephen Kiprotich at the London 2012 men's Olympic marathon](http://bit.ly/16kMKHQ)\n",
    "\n",
    "Using regression we want to predict a winning time for the next Olympic game.\n",
    "\n",
    "- graph showing gold medal times since 1896 (unit: min per km)\n",
    "- didnâ€™t have a standardised distance before 1924\n",
    "- badly organised in 1904 leading to very slow times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x188534b5fd0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGnCAYAAAAwtMlNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x188534d2278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(data['X'], data['Y'], 'r.',markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More Examples for Regression\n",
    "\n",
    "- predict quality of meat given spectral measurements (eg, Tecator Data)\n",
    "- radiocarbon dating, the C14 calibration curve: predict age given quantity of C14 isotope\n",
    "- predict quality of different Go or Backgammon moves given expert rated training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "$$y_i = m x_i + c$$\n",
    "\n",
    "Olympic example:\n",
    "\n",
    "- $y_i$ : winning time\n",
    "- $x_i$ : year of Olympics\n",
    "- $m$ : rate of improvement over time\n",
    "- $c$ : winning time at year 0\n",
    "\n",
    "Wikipedia: [Linear regression](https://en.wikipedia.org/wiki/Linear_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overdetermined and Underdetermined Systems\n",
    "\n",
    "Each unknown can be seen as an available degree of freedom, while each equation constrains the system, restricting one degree of freedom.\n",
    "\n",
    "##### Overdetermined system:\n",
    "\n",
    "- the system has been overconstrained\n",
    "- there are more equations than unknowns\n",
    "\n",
    "##### Underdetermined system:\n",
    "\n",
    "- the system has been underconstrained\n",
    "- the number of equations is fewer than the number of unknowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overdetermined System\n",
    "\n",
    "$$y = mx + c$$\n",
    "\n",
    "point 1: $x = 1$, $y=3$\n",
    "$$3 = m + c$$\n",
    "\n",
    "point 2: $x = 3$, $y=1$\n",
    "$$1 = 3m + c$$\n",
    "\n",
    "point 3: $x = 2$, $y=2.5$\n",
    "$$2.5 = 2m + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./figs/overdetermined.svg\", width=700, align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overdetermined System\n",
    "\n",
    "Introduction of a **slack variable** $\\epsilon$ :\n",
    "$$y = mx + c + \\epsilon$$\n",
    "\n",
    "point 1: $x = 1$, $y=3$\n",
    "$$3 = m + c + \\epsilon_1$$\n",
    "\n",
    "point 2: $x = 3$, $y=1$\n",
    "$$1 = 3m + c + \\epsilon_2$$\n",
    "\n",
    "point 3: $x = 2$, $y=2.5$ \n",
    "$$2.5 = 2m + c + \\epsilon_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review: Gaussian (Normal) Distribution\n",
    "\n",
    "$$\n",
    "  p(y | \\mu,\\sigma^2) \\buildrel\\triangle\\over = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y - \\mu)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "- denote this distribution as\n",
    "$$y \\sim \\mathcal{N}(\\mu,\\sigma^2)$$\n",
    "- $\\mu$ is the mean and $\\sigma^2$ is the variance\n",
    "- perhaps the most common/important probability distrution\n",
    "\n",
    "Wikipedia: [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Important Gaussian Properties\n",
    "\n",
    "**(1) Sum of Gaussians**\n",
    "\n",
    "The sum of Gaussian variables is also Gaussian.\n",
    "\n",
    "- suppose that $y_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)$, the sum is distributed as\n",
    "$$\\sum_{i=1}^n y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n \\mu_i, \\sum_{i=1}^n \\sigma_i^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(note) **central limit theorem**\n",
    "\n",
    "As sum increases, sum of non Gaussian, finite variance variables is also Gaussian.\n",
    "\n",
    "Wikipedia: [Central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Important Gaussian Properties\n",
    "\n",
    "**(2) Scaling a Gaussian**\n",
    "\n",
    "Scaling a Gaussian leads to a Gaussian.\n",
    "\n",
    "- suppose that $y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the scaled (by $w$) density is distributed as\n",
    "$$wy \\sim \\mathcal{N}(w\\mu, w^2\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Laplace's Idea\n",
    "\n",
    "Set the mean of Gaussian to be a function:\n",
    "$$\n",
    "  p(y_i|x_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i-f(x_i))^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "This gives us a *noisy function*.\n",
    "\n",
    "- known as a stochastic process\n",
    "\n",
    "Wikipedia: [Pierre-Simon Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Laplace's Idea\n",
    "\n",
    "- the standard Gaussian is parametised by mean and variance\n",
    "- make the mean a linear function of an *input*\n",
    "\n",
    "This leads to a regression model:\n",
    "$$\n",
    "  y_i = f(x_i) + \\epsilon_i = mx_i + c + \\epsilon_i\n",
    "$$\n",
    "where $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n",
    "\n",
    "- assume $y_i$ is height and $x_i$ is weight, ie, height is a function of weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood\n",
    "\n",
    "The likelihood of parameter values $\\theta$ given data $\\mathbf{x}$ is the probability for those observed data given those parameter values:\n",
    "$$\n",
    "  \\mathcal{L}(\\theta|x) = p(x|\\theta)\n",
    "$$\n",
    "\n",
    "Wikipedia: [Likelihood function](https://en.wikipedia.org/wiki/Likelihood_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Point Likelihood\n",
    "\n",
    "Likelihood of an individual data point $(x_i,y_i)$:\n",
    "$$\n",
    "  p(y_i | x_i,m,c,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i-mx_i-c)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "- parameters are gradient, $m$, offset, $c$, of the function and noise variance $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Set Likelihood\n",
    "\n",
    "Likelihood for a data set $(\\mathbf{x},\\mathbf{y})$:\n",
    "$$\n",
    "  p(\\mathbf{y} | \\mathbf{x},m,c,\\sigma^2) = \\prod_{i=1}^n p(y_i | x_i,m,c,\\sigma^2)\n",
    "$$\n",
    "for independent variables, ie, assume that\n",
    "\n",
    "- the noise, $\\epsilon_i$ is sampled independently for each data point\n",
    "- given $m$ and $c$, each data point is independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(note) **independent and identically distributed (i.i.d.) random variables**\n",
    "\n",
    "Imagine a set of data where each data point (random variable) is independently drawn from the same distribution.\n",
    "\n",
    "- each random variable has the same distribution as others\n",
    "- they would be mutually independent\n",
    "\n",
    "Wikipedia: [Independent and identically distributed random variables](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood\n",
    "\n",
    "A data set likelihood for **Normally distributed** random variables with the **i.i.d.** assumption:\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{y} | \\mathbf{x},m,c,\\sigma^2)\n",
    "  & = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i-mx_i-c)^2}{2\\sigma^2} \\right) \\\\\n",
    "  & = \\frac{1}{\\left(2\\pi\\sigma^2\\right)^{\\frac{n}{2}}} \\exp\\left( -\\frac{\\sum_{i=1}^n (y_i-mx_i-c)^2}{2\\sigma^2} \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood\n",
    "\n",
    "Often simpler if we work with the log likelihood:\n",
    "\\begin{align*}\n",
    "  \\mathcal{L}(m,c,\\sigma^{2})\n",
    "  & \\buildrel\\triangle\\over = \\log p(\\mathbf{y} | \\mathbf{x},m,c,\\sigma^2) \\\\\n",
    "  & = -\\frac{n}{2}\\log 2\\pi - \\frac{n}{2}\\log\\sigma^2 - \\sum_{i=1}^n \\frac{(y_i-mx_i-c)^2}{2\\sigma^2}\n",
    "\\end{align*}\n",
    "\n",
    "Wikipedia: [Likelihood function](https://en.wikipedia.org/wiki/Likelihood_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Estimation\n",
    "\n",
    "The aim of maximum likelihood estimation is to find the parameter values $\\theta$ that makes the observed data $\\mathbf{x}$ most likely:\n",
    "$$\n",
    "  \\mathop{\\rm argmax}_{\\theta} \\mathcal{L}(\\theta|x) = \\mathop{\\rm argmax}_{\\theta} p(x|\\theta)\n",
    "$$\n",
    "\n",
    "Wikipedia: [Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of Maximum Likelihood\n",
    "\n",
    "Mainstay of classical statistics:\n",
    "\n",
    "- If data was really generated according to probability we specified, correct parameters will be recovered in limit as $n \\rightarrow \\infty$.\n",
    "\n",
    "- This can be proven through sample based approximations (law of large numbers) of *KL divergences*.\n",
    "\n",
    "Wikipedia: [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(note) **law of large numbers**\n",
    "\n",
    "The number of i.i.d. random variables increases, their sample mean approaches their theoretical mean.\n",
    "\n",
    "(In other words, the number of trials increases, the outcomes will converge on the expected value.)\n",
    "\n",
    "Wikipedia: [Law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error Function\n",
    "\n",
    "Define an error function (objective function):\n",
    "$$\n",
    "  E(m,c,\\sigma^{2}) = \\frac{n}{2} \\log\\sigma^2 + \\sum_{i=1}^n \\frac{(y_i-mx_i-c)^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "*Learning* proceeds by minimising this error function for the data set provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error Function\n",
    "\n",
    "Probabilistic interpretation for minimising error function is equivalent to maximum likelihood with respect to parameters.\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}(m,c,\\sigma^{2}) = \\underbrace{-\\frac{n}{2}\\log 2\\pi}_{constant} - E(m,c,\\sigma^{2})\n",
    "$$\n",
    "\n",
    "- minimising error function is equivalent to maximising log likelihood\n",
    "- maximising log likelihood is equivalent to maximising the likelihood because the logarithm is monotonic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sum of Squares Error\n",
    "\n",
    "Ignoring terms which don't depend on $m$ and $c$ gives\n",
    "$$\n",
    "  E(m,c) = \\sum_{i=1}^n (y_i - f(x_i))^2\n",
    "$$\n",
    "where $f(x_i) = mx_i + c$.\n",
    "\n",
    "This is known as the **sum of squares** error function.\n",
    "\n",
    "- commonly used and is closely associated with the Gaussian likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical interpretation\n",
    "\n",
    "Error function expresses mismatch between prediction and reality:\n",
    "$$\n",
    "  E(m,c) = \\sum_{i=1}^n (y_i - f(x_i))^2\n",
    "$$\n",
    "\n",
    "Two functions are involved:\n",
    "\n",
    "- prediction function: $f(x_i) = mx_i + c$\n",
    "- error, or objective, function: $E(m,c)$\n",
    "\n",
    "Error function depends on parameters, $m$ and $c$, through prediction function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "Learning is minimisation of the objective function.\n",
    "\n",
    "- at the minima the gradient is zero\n",
    "\n",
    "**Coordinate descent**: find gradient in each coordinate and set to zero.\n",
    "\n",
    "Wikipedia: [Coordinate descent](https://en.wikipedia.org/wiki/Coordinate_descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "The gradient of $E(m,c)$ along the offset $c$:\n",
    "$$\n",
    "  \\frac{\\partial E(m,c)}{\\partial c} = -2\\sum_{i=1}^n (y_i - mx_i - c)\n",
    "$$\n",
    "then set $\\frac{\\partial E(m,c)}{\\partial c} = 0$, resulting in\n",
    "$$\n",
    "  c^* = \\frac{\\sum_{i=1}^n (y_i - mx_i)}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "The gradient of $E(m,c)$ along the slope $m$:\n",
    "$$\n",
    "  \\frac{\\partial E(m,c)}{\\partial m} = -2\\sum_{i=1}^n x_i(y_i - mx_i - c)\n",
    "$$\n",
    "then set $\\frac{\\partial E(m,c)}{\\partial m} = 0$, resulting in\n",
    "$$\n",
    "  m^* = \\frac{\\sum_{i=1}^n x_i(y_i - c)}{\\sum_{i=1}^n x_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed Point Updates\n",
    "\n",
    "\\begin{align*}\n",
    "  c^* = & \\frac{1}{n}\\sum_{i=1}^n (y_i - mx_i) \\\\\n",
    "  m^* = & \\frac{\\sum_{i=1}^n x_i(y_i - c^*)}{\\sum_{i=1}^n x_i^2} \\\\\n",
    "  (\\sigma^2)^* = & \\frac{1}{n} \\sum_{i=1}^n (y_i - m^* x_i - c^*)^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-dimensional Inputs\n",
    "\n",
    "There might be multiple factors that could contribute the outcome.\n",
    "Place these factors in a feature vector $\\mathbf{x}_i$ :\n",
    "$$\n",
    "  \\mathbf{x}_i = \\left( \\begin{array}{c} x_{i1} \\\\ \\vdots \\\\ x_{ip} \\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Linear function is now defined as\n",
    "$$\n",
    "f(\\mathbf{x}_i) = \\sum_{j=1}^p w_j x_{ij} + c\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Notation\n",
    "\n",
    "Now define a weight vector $\\mathbf{w}$ :\n",
    "$$\n",
    "  \\mathbf{w} = \\left( \\begin{array}{c} w_1 \\\\ \\vdots \\\\ w_p \\end{array} \\right)\n",
    "$$\n",
    "so that we write linear function in vector notation:\n",
    "$$\n",
    "  f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i + c\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Notation\n",
    "\n",
    "Can absorb $c$ into $\\mathbf{w}$ by assuming extra input $x_0$ which is always 1, ie,\n",
    "$$\n",
    "  f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i\n",
    "$$\n",
    "where\n",
    "$$\n",
    "  \\qquad\n",
    "  \\mathbf{w} = \\left( \\begin{array}{c} c \\\\ w_1 \\\\ \\vdots \\\\ w_p \\end{array} \\right) \\ ,\n",
    "  \\qquad\n",
    "  \\mathbf{x}_i = \\left( \\begin{array}{c} 1 \\\\ x_{i1} \\\\ \\vdots \\\\ x_{ip} \\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Regression: Likelihood\n",
    "\n",
    "Likelihood of a single **data point**:\n",
    "$$\n",
    "  p(y_i | \\mathbf{x}_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Log likelihood for a **data set**:\n",
    "\\begin{align*}\n",
    "  \\mathcal{L}(\\mathbf{w},\\sigma^2) = & \\log\\prod_{i=1}^n p(y_i | \\mathbf{x}_i) \\\\\n",
    "  = & - \\frac{n}{2}\\log 2\\pi - \\frac{n}{2}\\log\\sigma^2 - \\sum_{i=1}^n \\frac{(y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2}{2\\sigma^2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Regression: Error Function\n",
    "\n",
    "\\begin{align*}\n",
    "  & E(\\mathbf{w},\\sigma^2) \\\\\n",
    "  & = \\frac{n}{2}\\log\\sigma^2 + \\sum_{i=1}^n \\frac{(y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2}{2\\sigma^2} \\\\\n",
    "  & = \\frac{n}{2}\\log\\sigma^2 + \\frac{1}{2\\sigma^2} \\sum_{i=1}^n y_i^2 - \\frac{1}{\\sigma^2} \\sum_{i=1}^n y_i \\mathbf{w}^\\top \\mathbf{x}_i + \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\mathbf{w}^\\top \\mathbf{x}_i \\mathbf{x}_i^\\top \\mathbf{w} \\\\\n",
    "  & = \\frac{n}{2}\\log\\sigma^2 + \\frac{1}{2\\sigma^2} \\sum_{i=1}^n y_i^2 - \\frac{1}{\\sigma^2} \\mathbf{w}^\\top \\sum_{i=1}^n \\mathbf{x}_i y_i + \\frac{1}{2\\sigma^2} \\mathbf{w}^\\top \\left(\\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^\\top \\right)\\mathbf{w}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review: Multivariate Derivatives\n",
    "\n",
    "Recall that, for vectors $\\mathbf{a}$, $\\mathbf{w}$ and a square matrix $\\mathbf{A}$ :\n",
    "\\begin{align*}\n",
    "  \\frac{\\partial \\mathbf{a}^\\top \\mathbf{w}}{\\partial \\mathbf{w}} = & \\mathbf{a} \\\\\n",
    "  \\frac{\\partial \\mathbf{w}^\\top \\mathbf{A} \\mathbf{w}}{\\partial \\mathbf{w}} = & (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{w}\n",
    "\\end{align*}\n",
    "If $\\mathbf{A}$ is symmetric (ie, $\\mathbf{A} = \\mathbf{A}^\\top$) :\n",
    "$$\n",
    "  \\frac{\\partial \\mathbf{w}^\\top \\mathbf{A} \\mathbf{w}}{\\partial \\mathbf{w}} = 2\\mathbf{A}\\mathbf{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coordinate Descent\n",
    "\n",
    "Differentiate with respect to the vector $\\mathbf{w}$,\n",
    "\n",
    "$$\n",
    "  \\frac{\\partial E(\\mathbf{w},\\sigma^2)}{\\partial \\mathbf{w}} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\mathbf{x}_i y_i - \\frac{1}{\\sigma^2} \\left(\\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^\\top\\right) \\mathbf{w}\n",
    "$$\n",
    "then set $\\frac{\\partial E(\\mathbf{w},\\sigma^2)}{\\partial \\mathbf{w}} = 0$ and get\n",
    "$$\n",
    "  \\mathbf{w}^* = \\left(\\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^\\top\\right)^{-1} \\sum_{i=1}^n \\mathbf{x}_i y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Update Equations\n",
    "\n",
    "Rewrite in matrix notation:\n",
    "\\begin{align*}\n",
    "  \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^\\top \\buildrel\\triangle\\over = & \\mathbf{X}^\\top \\mathbf{X} \\\\\n",
    "  \\sum_{i=1}^n \\mathbf{x}_i y_i \\buildrel\\triangle\\over = & \\mathbf{X}^\\top \\mathbf{y}\n",
    "\\end{align*}\n",
    "Find updates for $\\mathbf{w}$ and $\\sigma^2$ :\n",
    "\\begin{align*}\n",
    "  \\mathbf{w}^* = & (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\\\\n",
    "  (\\sigma^2)^* = & \\frac{1}{n} \\sum_{i=1}^n (y_i - (\\mathbf{w}^*)^\\top \\mathbf{x}_i)^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important Concepts Not Covered\n",
    "\n",
    "- other optimization methods:\n",
    "\n",
    "  - second order methods, conjugate gradient, quasi-Newton and Newton\n",
    "  - effective heuristics such as momentum\n",
    "\n",
    "- local vs global solutions"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
