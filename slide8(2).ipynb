{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### week 8\n",
    "\n",
    "#  Constrained Optimisation and Maximum Entropy Modelling\n",
    "\n",
    "- entropy and maximum entropy modelling\n",
    "- constrained optimisation using Lagrange mutiplier\n",
    "- analytical solution and numerical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import notebook as nb\n",
    "import mlai\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review and Preview\n",
    "\n",
    "- last week we Looked at unsupervised learning\n",
    "- clustering, dimensionality reduction and latent variables and clustering were introduced\n",
    "- the topic this week concerns constrained optimisation for a maximum entropy model, which plays an important role in the field of information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy: Definition\n",
    "\n",
    "We calculate the entropy $H$ by\n",
    "$$\n",
    "  H = \\sum_{i = 1 \\ldots m} p_i \\log_b \\frac{1}{p_i} = - \\sum_{i = 1 \\ldots m} p_i \\log_b p_i\n",
    "$$\n",
    "\n",
    "- Typical choices for the base $b$ are $2$, $e$, $10$\n",
    "- Suppose the log is taken to the base $2$ (ie, $b=2$), entropy is expressed in *bits*\n",
    "- We drop $b$ and use **natural log** with the base $e$, unless otherwise noted\n",
    "- we may sometime write $H(p)$ for the above quantity\n",
    "- as a convention, $\\displaystyle 0\\log \\frac{1}{0} = 0$, because $\\displaystyle \\lim_{a\\rightarrow 0+} a\\log \\frac{1}{a} \\rightarrow 0$\n",
    "- $H(X) = \\sum_{x\\in {\\cal X}} p(x) \\log \\frac{1}{p(x)}$ for a random variable $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy: Examples\n",
    "\n",
    "1. a flip of a fair coin:\n",
    "$$\n",
    "  H = \\frac{1}{2}\\log_2 2 + \\frac{1}{2}\\log_2 2 = 1 \\ (\\text{bit})\n",
    "$$\n",
    "\n",
    "2. a biased coin, with head coming up as twice frequently as tail:\n",
    "$$\n",
    "  H = \\frac{2}{3}\\log_2\\frac{3}{2} + \\frac{1}{3}\\log_2 3 \\sim 0.92 \\ (\\text{bits})\n",
    "$$\n",
    "\n",
    "3. a race between four horses, having a chance to win with probabilities $0.4$, $0.3$, $0.2$, and $0.1$:\n",
    "$$\n",
    "  H = 0.4\\log_2 \\frac{1}{0.4} + 0.3\\log_2 \\frac{1}{0.3} + 0.2\\log_2 \\frac{1}{0.2} + 0.1\\log_2 \\frac{1}{0.1} \\sim 1.85 \\ (\\text{bits})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coin Tossing\n",
    "\n",
    "Suppose a weighted coin has probability $h$ of coming up heads,\tentropy of tossing the coin only once is given by\n",
    "$$\n",
    "  H(X) = h \\log_2 \\frac{1}{h} + (1 - h) \\log_2 \\frac{1}{1 - h}\n",
    "$$\n",
    "\n",
    "Entropy is maximised when the coin is fair (ie, unbiased):\n",
    "<img src=\"./figs/coin_toss.jpg\", width=300, align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model\n",
    "\n",
    "Concept:\n",
    "\n",
    "- model **all** that is known\n",
    "- assume **nothing** that is unknown\n",
    "\n",
    "Maximum entropy principle:\n",
    "\n",
    "- given a collection of facts, the maximum entropy method chooses a model that is **consistent with all facts**, but otherwise **as uniform as possible**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model: Simple Example\n",
    "\n",
    "We wish to estimate a joint probability distribution $p(x, y)$ where $x\\in\\{x_1, x_2\\}$ and $y\\in\\{y_1, y_2\\}$, given the constraints\n",
    "\\begin{align*}\n",
    "  p(x_1, y_1) + p(x_2, y_1) &= 0.6 \\\\\n",
    "  p(x_1, y_1) + p(x_1, y_2) + p(x_2, y_1) + p(x_2, y_2) &= 1\n",
    "\\end{align*}\n",
    "Given these constraints, our objective is to maximise\n",
    "$$\n",
    "  H(X, Y) = \\sum_{x\\in\\{x_1, x_2\\}} \\sum_{y\\in\\{y_1, y_2\\}} p(x, y) \\log\\frac{1}{p(x, y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One distribution that satisfies the constrains:\n",
    "\n",
    ".     | $y_1$ | $y_2$ |\n",
    ":----:|:-----:|:-----:|:-----:\n",
    "$x_1$ | 0.5   | 0.1   |\n",
    "$x_2$ | 0.1   | 0.3   |\n",
    "total | 0.6   |       | 1.0\n",
    "\n",
    "The most uniform distribution that satisfies the constrains:\n",
    "\n",
    ".     | $y_1$ | $y_2$ |\n",
    ":----:|:-----:|:-----:|:-----:\n",
    "$x_1$ | 0.3   | 0.2   |\n",
    "$x_2$ | 0.3   | 0.2   |\n",
    "total | 0.6   |       | 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Example) Weather Forecast: Problem Description\n",
    "\n",
    "Firstly we make overly simplified assumption that five weather types\n",
    "$$\n",
    "  \\{misty, foggy, cloudy, sunny, rainy\\}\n",
    "$$\n",
    "can fully describe weather.\n",
    "\n",
    "Now, suppose that today's weather is $\\{cloudy\\}$, what will be tomorrow's weather?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Forecast: Single Constraint (I)\n",
    "\n",
    "Initially we have the **total probability** constraint:\n",
    "$$\n",
    "  p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) = 1\n",
    "$$\n",
    "There exist infinite combinations that may satisfy the constraint above.\n",
    "\n",
    "The most intuitively appealing model is the one that allocates the total probability evenly among five possible weathers:\n",
    "\\begin{align*}\n",
    "  p(misty) &= 0.2 \\\\\n",
    "  p(foggy) &= 0.2 \\\\\n",
    "  p(cloudy) &= 0.2 \\\\\n",
    "  p(sunny) &= 0.2 \\\\\n",
    "  p(rainy) &= 0.2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Forecast: Single Constraint (II)\n",
    "\n",
    "Analytically, we wish to maximise the entropy $H(Y)$ given the total probability constraint.\n",
    "Using a **Lagrange multiplier** $\\lambda_1$ :\n",
    "$$\n",
    "  \\Lambda = H(Y) + \\lambda_1 \\times \\text{constraint} = \\left( - \\sum_{y\\in {\\cal Y}} p(y) \\log p(y) \\right) + \\lambda_1 \\left( \\sum_{y\\in {\\cal Y}} p(y) - 1 \\right)\n",
    "$$\n",
    "and calculate the partial derivative with respect to $p(y)$ :\n",
    "$$\n",
    "  \\frac{\\partial\\Lambda}{\\partial p(y)} = - \\log p(y) - 1 + \\lambda_1\n",
    "$$\n",
    "then set $\\frac{\\partial\\Lambda}{\\partial p(y)} = 0$, resulting in $p(y) = 0.2$ for all $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Forecast: Two Constraints (I)\n",
    "\n",
    "Suppose we have two constraints:\n",
    "\\begin{align*}\n",
    "  p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) &= 1 \\\\\n",
    "  p(misty) + p(foggy) &= 0.3\n",
    "\\end{align*}\n",
    "By observation the most uniform model is\n",
    "\\begin{align*}\n",
    "  p(misty) &= 0.15 \\\\\n",
    "  p(foggy) &= 0.15 \\\\\n",
    "  p(cloudy) &= 0.233... \\\\\n",
    "  p(sunny) &= 0.233... \\\\\n",
    "  p(rainy) &= 0.233...\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Forecast: Two Constraints (II)\n",
    "\n",
    "Analytically, we maximise $H(Y)$ given those two constraints:\n",
    "\\begin{align*}\n",
    "  \\Lambda = H(Y) & + \\lambda_1 \\{ p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) - 1 \\} \\\\\n",
    "  & + \\lambda_2 \\{ p(misty) + p(foggy) - 0.3 \\}\n",
    "\\end{align*}\n",
    "and calculate partial derivatives:\n",
    "$$\n",
    "  \\frac{\\partial\\Lambda}{\\partial p(y)} = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "      - \\log p(y) - 1 + \\lambda_1 + \\lambda_2\t& y = misty, foggy \\\\\n",
    "      - \\log p(y) - 1 + \\lambda_1\t\t& \\text{otherwise}\n",
    "\t\\end{array} \\right.\n",
    "$$\n",
    "Then set $\\frac{\\partial\\Lambda}{\\partial p(y)} = 0$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Forecast: Three Constraints (I)\n",
    "\n",
    "Suppose we have *three* constraints:\n",
    "\\begin{align*}\n",
    "  p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) &= 1 \\\\\n",
    "  p(misty) + p(foggy) &= 0.3 \\\\\n",
    "  p(misty) + p(cloudy) &= 0.5\n",
    "\\end{align*}\n",
    "\n",
    "Solution is no longer obvious, but we still can work on this case analytically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Forecast: Three Constraints (II)\n",
    "\n",
    "We maximise $H(Y)$ given three constraints:\n",
    "\\begin{align*}\n",
    "  \\Lambda = H(Y) & + \\lambda_1 \\{ p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) - 1 \\} \\\\\n",
    "  & + \\lambda_2 \\{ p(misty) + p(foggy) - 0.3 \\} \\\\\n",
    "  & + \\lambda_3 \\{ p(misty) + p(cloudy) - 0.5 \\}\n",
    "\\end{align*}\n",
    "The partial derivatives are\n",
    "$$\n",
    "  \\frac{\\partial\\Lambda}{\\partial p(y)} = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "      - \\log p(y) - 1 + \\lambda_1 + \\lambda_2 + \\lambda_3 & y = misty \\\\\n",
    "      - \\log p(y) - 1 + \\lambda_1 + \\lambda_2 & y = foggy \\\\\n",
    "      - \\log p(y) - 1 + \\lambda_1 + \\lambda_3 & y = cloudy \\\\\n",
    "      - \\log p(y) - 1 + \\lambda_1 & \\text{otherwise}\n",
    "\t\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Forecast: Three Constraints (III)\n",
    "\n",
    "We set $\\frac{\\partial\\Lambda}{\\partial p(y)} = 0$ and get the most uniform model:\n",
    "\\begin{align*}\n",
    "  p(misty) &= \\frac{9 - \\sqrt{51}}{10} = 0.186... \\\\\n",
    "  p(foggy) &= \\frac{\\sqrt{51} - 6}{10} = 0.114... \\\\\n",
    "  p(cloudy) &= \\frac{\\sqrt{51} - 4}{10} = 0.314... \\\\\n",
    "  p(sunny) &= \\frac{11 - \\sqrt{51}}{20} = 0.193... \\\\\n",
    "  p(rainy) &= \\frac{11 - \\sqrt{51}}{20} = 0.193...\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model (I)\n",
    "\n",
    "Formally we define a **random process** as follows:\n",
    "\\begin{align*}\n",
    "  x &: \\text{some information influencing the output}, \\ x\\in{\\cal X} \\\\\n",
    "  y &: \\text{output value}, \\ y\\in{\\cal Y}\n",
    "\\end{align*}\n",
    "\n",
    "(eg) a random process defined for the **weather forecast** problem:\n",
    "\\begin{align*}\n",
    "  x &: \\text{today's weather}, \\ x\\in\\{cloudy\\} \\\\\n",
    "  y &: \\text{tomorrow's weather}, \\ y\\in\\{misty, foggy, cloudy, sunny, rainy\\}\n",
    "\\end{align*}\n",
    "\n",
    "Wikipedia: [Principle of maximum entropy](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model (II)\n",
    "\n",
    "We also have $N$ **training samples** $(x_1, y_1), (x_2, y_2), \\ \\ldots \\ , (x_N, y_N)$ .\n",
    "\n",
    "(eg) ten training samples for the **weather forecast** problem:\n",
    "\\begin{align*}\n",
    "  & (cloudy, cloudy), (cloudy, sunny), (cloudy, sunny), (cloudy, misty), (cloudy, cloudy), \\\\\n",
    "  & (cloudy, rainy), (cloudy, misty), (cloudy, foggy), (cloudy, cloudy), (cloudy, rainy)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model (III)\n",
    "\n",
    "For $i = 1,\\ldots,n$ ($n$: number of **features**), we define $f_i(x, y)$, an **indicator function** of type ${\\cal X}\\times{\\cal Y}\\longrightarrow\\{0, 1\\}$ .\n",
    "\n",
    "(eg) a feature set for the **weather forecast** problem:\n",
    "\\begin{align*}\n",
    "  f_1(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, foggy, cloudy, sunny, rainy\\} \\\\\n",
    "  f_2(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, foggy\\} \\\\\n",
    "  f_3(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, cloudy\\}\n",
    "\\end{align*}\n",
    "otherwise $f_i(x, y) = 0$ for $i = 1,2,3$ .\n",
    "\n",
    "- note that $x$ is always $\\{cloudy\\}$ with this problem\n",
    "- if one sample is $(cloudy, foggy)$: $f_1 = 1, f_2 = 1, f_3 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model (IV)\n",
    "\n",
    "The **expected value** of $f_i$ with respect to an **empirical distribution** $\\tilde{p}(x, y)$ is given by\n",
    "$$\n",
    "  \\tilde{p}(f_i) \\equiv E_{\\tilde{p}}[f_i] = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x, y) f_i(x, y)\n",
    "$$\n",
    "\n",
    "- $\\tilde{p}(x, y)$ represents a summary of the training sample, that is,\n",
    "$$\n",
    "  \\tilde{p}(x, y) \\equiv \\frac{1}{N} \\times \\text{number of times that $(x, y)$ occurs in the sample}\n",
    "$$\n",
    "\n",
    "- some pair $(x, y)$ may not occur at all in the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model (V)\n",
    "\n",
    "The **expected value** of $f_i$ with respect to a **model distribution** $p(x, y)$ is\n",
    "$$\n",
    "  p(f_i) \\equiv E_p[f_i] = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} p(x, y) f_i(x, y) \\sim \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p(y | x) f_i(x, y)\n",
    "$$\n",
    "\n",
    "- calculation of $p(f_i)$ with respect to $p(x, y)$ is to the order of $|\\ {\\cal X}\\times{\\cal Y}\\ |$, which is often too large\n",
    "- by using the empirical distribution $\\tilde{p}(x)$, the calculation gets more tractable because we only consider the training sample\n",
    "- we are likely to have more reliable estimates for $p(y | x)$ than for $p(x, y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model (VI)\n",
    "\n",
    "Now we have the following **constraint** that relates two expected values:\n",
    "$$\n",
    "  p(f_i) = \\tilde{p}(f_i)\n",
    "$$\n",
    "- $\\tilde{p}(f_i)$ represents statistical phenomena in the training sample\n",
    "- $p(f_i) = \\tilde{p}(f_i)$ is the mean of requiring that our model generalises the phenomena\n",
    "\n",
    "(eg) constraints for the **weather forecast** problem:\n",
    "\\begin{align*}\n",
    "  p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) &= 1 \\\\\n",
    "  p(misty) + p(foggy) &= 0.3 \\\\\n",
    "  p(misty) + p(cloudy) &= 0.5\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model (VII)\n",
    "\n",
    "Of all conditional probability distributions ${\\cal P}$, a subset\n",
    "$$\n",
    "  {\\cal C} \\equiv \\{ \\ p\\in{\\cal P} \\ | \\ p(f_i) = \\tilde{p}(f_i) \\ \\text{ for } \\ i = 1,\\ldots,n \\ \\}\n",
    "$$\n",
    "constrains the model according to our knowledge.\n",
    "\n",
    "A **mathematical measure of the uniformity** is the conditional entropy:\n",
    "$$\n",
    "  H_p(Y|X) \\equiv - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p(y | x) \\log p(y | x)\n",
    "$$\n",
    "where the notation $H_p$ emphasises the dependency on $p$ .\n",
    "\n",
    "Finally we have reached the **maximum entropy model** of the form:\n",
    "$$\n",
    "  p^* = \\mathop{\\rm argmax}_{p\\in{\\cal C}} H_p(Y|X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model: Analytical Solution (I)\n",
    "\n",
    "For each feature $f_i \\ (i = 1,\\ldots,n)$, we introduce a **Lagrange multiplier** $\\lambda_i$, then define the **Lagrangian**:\n",
    "$$\n",
    "  \\Lambda(p, \\lambda) \\equiv H_p(Y|X) + \\sum_i \\lambda_i \\{ p(f_i) - \\tilde{p}(f_i) \\}\n",
    "$$\n",
    "    \n",
    "Holding $\\lambda = \\{\\lambda_i\\}$ fixed, we compute the unconstrained maximum of $\\Lambda(p, \\lambda)$ over all $p\\in{\\cal P}$ :\n",
    "$$\n",
    "  p_{\\lambda}(y|x) = \\mathop{\\rm argmax}_{p\\in{\\cal P}} \\Lambda(p, \\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model: Analytical Solution (II)\n",
    "\n",
    "General solution is the exponential model:\n",
    "$$\n",
    "  p_{\\lambda}(y|x) = \\frac{1}{Z_{\\lambda}(x)} \\exp \\left( \\sum_i \\lambda_i f_i(x,y) \\right)\n",
    "$$\n",
    "where $Z_{\\lambda}(x) = \\sum_{y\\in {\\cal Y}} \\exp \\left( \\sum_i \\lambda_i f_i(x,y) \\right)$ is a normalising constant.\n",
    "\n",
    "- technically, $\\lambda_i$ is a **Lagrange multiplier**, associated with the feature $f_i(x,y)$, in a certain constrained optimisation problem\n",
    "- in a sense, $\\lambda_i$ is a measure of the **importance** of the feature $f_i(x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model: Numerical Solution\n",
    "\n",
    "1. initially set $\\lambda_i = 0$ for all $i = 1,\\ldots,n$\n",
    "\n",
    "2. do for each $i = 1,\\ldots,n$ :\n",
    "\n",
    "   **A.** let $\\delta_i$ be the solution to\n",
    "   $$\n",
    "     \\displaystyle \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) f_i(x,y) \\exp\\left( \\delta_i f^{\\#}(x,y) \\right) = \\tilde{p}(f_i)\n",
    "   $$\n",
    "   where $f^{\\#}(x,y) \\equiv \\sum_{i = 1,\\ldots,n} f_i(x,y)$\n",
    "\n",
    "   **B.** update multipliers by $\\lambda_i \\leftarrow \\lambda_i + \\delta_i$\n",
    "\n",
    "3. go back to step 2, if not all $\\lambda_i$ have converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall: Weather Forecast with Three Constraints (I)\n",
    "\n",
    "We have *three* constraints:\n",
    "\\begin{align*}\n",
    "  p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) &= 1 \\\\\n",
    "  p(misty) + p(foggy) &= 0.3 \\\\\n",
    "  p(misty) + p(cloudy) &= 0.5\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall: Weather Forecast with Three Constraints (II)\n",
    "\n",
    "We reached the most uniform model analytically:\n",
    "\\begin{align*}\n",
    "  p(misty) &= 0.186... \\\\\n",
    "  p(foggy) &= 0.114... \\\\\n",
    "  p(cloudy) &= 0.314... \\\\\n",
    "  p(sunny) &= 0.193... \\\\\n",
    "  p(rainy) &= 0.193...\n",
    "\\end{align*}\n",
    "\n",
    "(next graph) Numerical calculation of the maxmum entropy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./figs/isa.png\", width=800, align=center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
