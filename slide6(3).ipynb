{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### week 6\n",
    "\n",
    "# Bayesian Regression\n",
    "\n",
    "- overdetermined and underdetermined systems\n",
    "- Bayesian inference: prior and posterior\n",
    "- multivariate Bayesian regression\n",
    "- Bayesian approach to making prediction with Olympic marathon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import notebook as nb\n",
    "import mlai\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "basis = mlai.polynomial\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "data_limits = [1892, 2020]\n",
    "num_data = x.shape[0]\n",
    "max_basis = y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review and Preview\n",
    "\n",
    "- Last week we studied an over-fitting problem in machine learning and explore how to validate the model\n",
    "- We tested hold out validation schemes, including leave one out error and $k$-fold cross validation\n",
    "- This week we look at Bayesian regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Simultaneous Equations\n",
    "\n",
    "Given two observations $(x_1,y_1)$ and $(x_2,y_2)$, a system of two simultaneous equations with two unknowns $m$ and $c$ :\n",
    "\\begin{align*}\n",
    "  y_1 = & mx_1 + c \\\\\n",
    "  y_2 = & mx_2 + c\n",
    "\\end{align*}\n",
    "is leading to a solution:\n",
    "\\begin{align*}\n",
    "  m^* = & \\frac{y_2-y_1}{x_2-x_1} \\\\\n",
    "  c^* = & y_1 - m^* x_1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overdetermined System (Review)\n",
    "\n",
    "How do we deal with three observations with only two unknowns?\n",
    "\\begin{align*}\n",
    "  y_1 = & mx_1 + c \\\\\n",
    "  y_2 = & mx_2 + c \\\\\n",
    "  y_3 = & mx_3 + c\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overdetermined System (Review)\n",
    "\n",
    "In ** week 3** we solved this problem through introduction of a Gaussian noise model, ie, given $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ :\n",
    "\\begin{align*}\n",
    "  y_1 = & mx_1 + c + \\epsilon_1 \\\\\n",
    "  y_2 = & mx_2 + c + \\epsilon_2 \\\\\n",
    "  y_3 = & mx_3 + c + \\epsilon_3\n",
    "\\end{align*}\n",
    "\n",
    "- a noise model to give mismatch between model and data\n",
    "- model justified by appeal to the **central limit theorem**\n",
    "- maximum likelihood with Gaussian noise leading to **least squares**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underdetermined System\n",
    "\n",
    "What about two unknowns but only one observation?\n",
    "$$\n",
    "  y_1 =  mx_1 + c\n",
    "$$\n",
    "\n",
    "(next graph) lines through a single point $(1,3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./diagrams/under_determined_system000.svg'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_plots('under_determined_system{samp:0>3}.svg', directory='./diagrams', samp=(0, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underdetermined System\n",
    "\n",
    "We can compute $m$ given $c$ :\n",
    "$$\n",
    "  m^* = \\frac{y_1 - c}{x_1}\n",
    "$$\n",
    "where we may assume\n",
    "$$\n",
    "  c \\sim \\mathcal{N}(0,\\alpha_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior Distribution\n",
    "\n",
    "Bayesian inference requires a prior on the parameters.\n",
    "\n",
    "- the prior to represent our belief *before* we see the data of the likely value of the parameters\n",
    "- for linear regression, consider a Gaussian prior:\n",
    "$$\n",
    "  c \\sim \\mathcal{N}(0, \\alpha_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Posterior Distribution\n",
    "\n",
    "Posterior distribution is found by combining the prior with the likelihood.\n",
    "\n",
    "- posterior distribution to represent our belief *after* we see the data of the likely value of the parameters\n",
    "- the posterior being derived through **Bayes' rule**:\n",
    "$$\n",
    "  p(c|y) = \\frac{p(y|c)p(c)}{p(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior and Likelihood are Both Exponentiated Quadratics\n",
    "\n",
    "We consider a Gaussian prior $c \\sim \\mathcal{N}(0,\\alpha_1)$ :\n",
    "$$\n",
    "  p(c) = \\frac{1}{\\sqrt{2\\pi\\alpha_1}} \\exp\\left( -\\frac{c^2}{2\\alpha_1} \\right)\n",
    "$$\n",
    "\n",
    "We saw in **week 3** that, by using a linear regression model $y_i = mx_i + c + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ for a data set $(\\mathbf{x},\\mathbf{y})$, it has resulted in the following likelihood function:\n",
    "$$\n",
    "  p(\\mathbf{y} | \\mathbf{x},m,c,\\sigma^2) = \\frac{1}{\\left(2\\pi\\sigma^2\\right)^{\\frac{n}{2}}} \\exp\\left( -\\frac{\\sum_{i=1}^n (y_i-mx_i-c)^2}{2\\sigma^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stages to Derivation of the Posterior\n",
    "\n",
    "1. multiply likelihood by prior\n",
    "\n",
    "   - when they are both *exponentiated quadratics*, the answer is always also an *exponentiated quadratic* because $\\exp(a^2)\\exp(b^2) = \\exp(a^2 + b^2)$\n",
    "\n",
    "2. complete the square to get the resulting density in the form of a Gaussian\n",
    "\n",
    "3. recognise the mean and (co)variance of the Gaussian\n",
    "\n",
    "This is the estimate of the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiplying Likelihood by Prior\n",
    "\n",
    "The Bayes rule gives\n",
    "$$\n",
    "  p(c | \\mathbf{x},\\mathbf{y},m,\\sigma^2) = \\frac{p(\\mathbf{y} | \\mathbf{x},m,c,\\sigma^2) p(c)}{p(\\mathbf{y} | \\mathbf{x},m,\\sigma^2)}\n",
    "$$\n",
    "We now move to logarithm:\n",
    "$$\n",
    "  \\log p(c | \\mathbf{x},\\mathbf{y},m,\\sigma^2) = \\log p(\\mathbf{y} | \\mathbf{x},m,c,\\sigma^2) + \\log p(c) - \\underbrace{\\log p(\\mathbf{y} | \\mathbf{x},m,\\sigma^2)}_{can\\ be\\ ignored}\n",
    "$$\n",
    "The last term can be ignored because\n",
    "$$\n",
    "  \\text{argmax}_c \\log p(c | \\mathbf{x},\\mathbf{y},m,\\sigma^2) = \\text{argmax}_c \\{\\log p(\\mathbf{y} | \\mathbf{x},m,c,\\sigma^2) + \\log p(c)\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now complete the square of the quadratic form to obtain\n",
    "\\begin{align*}\n",
    "  \\log p(c | \\mathbf{x},\\mathbf{y},m,\\sigma^2)\n",
    "  & = \\underbrace{\\log\\frac{1}{\\left(2\\pi\\sigma^2\\right)^{\\frac{n}{2}}}}_{constant} - \\frac{\\sum_{i=1}^n (y_i-mx_i-c)^2}{2\\sigma^2} + \\underbrace{\\log\\frac{1}{\\sqrt{2\\pi\\alpha_1}}}_{constant} - \\frac{c^2}{2\\alpha_1} - \\underbrace{\\log p(\\mathbf{y} | \\mathbf{x},m,\\sigma^2)}_{can\\ be\\ ignored} \\\\\n",
    "  & = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i-mx_i)^2 + c \\sum_{i=1}^n \\frac{(y_i-mx_i)}{\\sigma^2} - c^2 \\left( \\frac{n}{2\\sigma^2} + \\frac{1}{2\\alpha_1} \\right) + const. \\\\\n",
    "  & \\buildrel\\triangle\\over = -\\frac{1}{2\\tau^2}(c-\\mu)^2 + const.\n",
    "\\end{align*}\n",
    "where $\\tau^2 = \\left( n\\sigma^{-2} + \\alpha_1^{-1} \\right)^{-1}$ and $\\mu = \\frac{\\tau^2}{\\sigma^2} \\sum_{i=1}^n (y_i-mx_i)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Update\n",
    "\n",
    "Why would we like to use a Gaussian prior with a Gaussian likelihood?\n",
    "\n",
    "(note) **conjugate prior**\n",
    "\n",
    "'In Bayesian probability theory, if the posterior distributions $p(\\theta|x)$ are in the same family as the prior probability distribution $p(\\theta)$, the prior and posterior are then called conjugate distributions, and the prior is called a **conjugate prior** for the **likelihood function**.'\n",
    "\n",
    "from Wikipedia: [Conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior)\n",
    "\n",
    "(next graph) posterior distribution is created from likelihood multiplied by prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./diagrams/dem_gaussian03.svg'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_plots('dem_gaussian{stage:0>2}.svg', './diagrams', stage=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Joint Density\n",
    "\n",
    "We really want to know the *joint* posterior density over the parameters $c$ *and* $m$.\n",
    "\n",
    "We could now integrate out over $m$, but it is easier to consider the multivariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bivariate Model with Height and Weight\n",
    "\n",
    "![](./diagrams/height_weight_gaussian.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Independence Assumption\n",
    "\n",
    "We may assume that height and weight are independent: $p(h,w) = p(h)p(w)$\n",
    "\n",
    "Wikipedia: [Statistical assumption](https://en.wikipedia.org/wiki/Statistical_assumption)\n",
    "\n",
    "(next graph) assuming independence of height and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./diagrams/independent_height_weight079.png'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_plots('independent_height_weight{fig:0>3}.png', './diagrams', fig=(0,79))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Independent Gaussians\n",
    "\n",
    "\\begin{align*}\n",
    "  p(w, h)\n",
    "  = & p(w)p(h) \\\\\n",
    "  = & \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp\\left( -\\frac{(w-\\mu_1)^2}{2\\sigma_1^2} \\right) \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma_2^2}} \\exp\\left( -\\frac{(w-\\mu_2)^2}{2\\sigma_2^2} \\right) \\\\\n",
    "  = & \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}\\sqrt{2\\pi\\sigma_2^2}} \\exp\\left( -\\frac{1}{2} \\left( \\frac{(w-\\mu_1)^2}{\\sigma_1^2} + \\frac{(h-\\mu_2)^2}{\\sigma_2^2} \\right) \\right) \\\\\n",
    "  = & \\frac{1}{\\sqrt{2\\pi\\sigma_1^2 2\\pi\\sigma_2^2}} \\exp\\left( -\\frac{1}{2} \\left( \\begin{bmatrix} w \\\\ h \\end{bmatrix} - \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix} \\right)^\\top \\begin{bmatrix} \\sigma_1^2 & 0 \\\\ 0 & \\sigma_2^2 \\end{bmatrix}^{-1}\\left( \\begin{bmatrix} w \\\\ h \\end{bmatrix} - \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix} \\right) \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally we come to\n",
    "$$\n",
    "  p(\\mathbf{y}) = \\frac{1}{\\left| 2\\pi\\mathbf{D} \\right|^{\\frac{1}{2}}} \\exp\\left( -\\frac{1}{2}(\\mathbf{y} - \\boldsymbol{\\mu})^\\top \\mathbf{D}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu}) \\right)\n",
    "$$\n",
    "by defining\n",
    "$$\n",
    "  \\mathbf{y} = \\begin{bmatrix} w \\\\ h \\end{bmatrix} \\qquad \\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix} \\qquad \\mathbf{D} = \\begin{bmatrix} \\sigma_1^2 & 0 \\\\ 0 & \\sigma_2^2 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlated Variables\n",
    "\n",
    "In reality height and weight are not independent.\n",
    "\n",
    "- body mass index (BMI) $\\sim \\frac{w}{h^2}$\n",
    "\n",
    "(next graph) height and weight are correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./diagrams/correlated_height_weight079.png'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_plots('correlated_height_weight{fig:0>3}.png', './diagrams', fig=(0,79))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlated Gaussian\n",
    "\n",
    "The form is now correlated by rotating the data space using matrix $\\mathbf{R}$ :\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{y})\n",
    "  = & \\frac{1}{\\left| 2\\pi\\mathbf{D} \\right|^{\\frac{1}{2}}} \\exp\\left( -\\frac{1}{2}(\\mathbf{R}^\\top\\mathbf{y} - \\mathbf{R}^\\top\\boldsymbol{\\mu})^\\top \\mathbf{D}^{-1} (\\mathbf{R}^\\top\\mathbf{y} - \\mathbf{R}^\\top\\boldsymbol{\\mu}) \\right) \\\\\n",
    "  = & \\frac{1}{\\left| 2\\pi\\mathbf{D} \\right|^{\\frac{1}{2}}} \\exp\\left( -\\frac{1}{2}(\\mathbf{y} - \\boldsymbol{\\mu})^\\top \\mathbf{R}\\mathbf{D}^{-1}\\mathbf{R}^\\top (\\mathbf{y} - \\boldsymbol{\\mu}) \\right) \\\\\n",
    "  = & \\frac{1}{\\left| 2\\pi\\mathbf{C} \\right|^{\\frac{1}{2}}} \\exp\\left( -\\frac{1}{2}(\\mathbf{y} - \\boldsymbol{\\mu})^\\top \\mathbf{C}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu}) \\right)\n",
    "\\end{align*}\n",
    "where $\\mathbf{C}^{-1} = \\mathbf{R} \\mathbf{D}^{-1} \\mathbf{R}^\\top$, which gives a covariance matrix $\\mathbf{C} = \\mathbf{R} \\mathbf{D} \\mathbf{R}^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Bayesian Approach: the Model\n",
    "\n",
    "For a set of $n$ data points, $(\\mathbf{x},\\mathbf{y}) = \\{(x_1,y_1), \\ldots, (x_n,y_n)\\}$, we start with a regression model with basis function $\\boldsymbol{\\Phi}$ :\n",
    "$$\n",
    "  \\mathbf{y} = f(\\mathbf{x}) + \\boldsymbol{\\epsilon} = \\boldsymbol{\\Phi}\\mathbf{w} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "  \\mathbf{y} = \\left(\\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array}\\right)\n",
    "  \\qquad\n",
    "  \\boldsymbol{\\Phi} = \\left(\\begin{array}{c}\n",
    "    \\boldsymbol{\\phi}_1^\\top \\\\\n",
    "    \\vdots \\\\\n",
    "    \\boldsymbol{\\phi}_n^\\top\n",
    "  \\end{array}\\right) = \\left(\\begin{array}{ccc}\n",
    "    \\phi_0(x_1) & \\dots & \\phi_m(x_1) \\\\\n",
    "    \\vdots      &       & \\vdots \\\\\n",
    "    \\phi_0(x_n) & \\dots & \\phi_m(x_n)\n",
    "  \\end{array}\\right)\n",
    "  \\qquad\n",
    "  \\mathbf{w} = \\left(\\begin{array}{c} w_0 \\\\ \\vdots \\\\ w_m \\end{array}\\right)\n",
    "  \\qquad\n",
    "  \\boldsymbol{\\epsilon} = \\left(\\begin{array}{c} \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right)\n",
    "$$\n",
    "(reveiw) **week 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Approach:  Sampling the Prior\n",
    "\n",
    "For a regression model, $\\mathbf{y} = \\boldsymbol{\\Phi}\\mathbf{w} + \\boldsymbol{\\epsilon}$ :\n",
    "\n",
    "- assume Gaussian noise (**week 3**) for $\\boldsymbol{\\epsilon}$:\n",
    "$$\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I})$$\n",
    "\n",
    "- sample the Gaussian prior for $\\mathbf{w}$:\n",
    "$$\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{0}, \\alpha\\mathbf{I})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Approach: Sampling the Posterior\n",
    "\n",
    "Recall the Bayes rule,\n",
    "$$\n",
    "  p(\\mathbf{w} | \\boldsymbol{\\Phi},\\mathbf{y},\\sigma^2) = \\frac{p(\\mathbf{y} | \\boldsymbol{\\Phi},\\mathbf{w},\\sigma^2) p(\\mathbf{w})}{p(\\mathbf{y} | \\boldsymbol{\\Phi},\\sigma^2)}\n",
    "$$\n",
    "and we are now given the prior,\n",
    "$$\n",
    "  p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{0}, \\alpha\\mathbf{I})\n",
    "$$\n",
    "After some effort, it eventually leads to the posteror,\n",
    "$$\n",
    "  p(\\mathbf{w} | \\boldsymbol{\\Phi},\\mathbf{y},\\sigma^2) = \\mathcal{N}(\\boldsymbol{\\mu}_w,\\mathbf{C}_w)\n",
    "$$\n",
    "where\n",
    "\\begin{align*}\n",
    "  \\mathbf{C}_w = & \\left( \\sigma^{-2} \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} + \\alpha^{-1} \\mathbf{I} \\right)^{-1} \\\\\n",
    "  \\boldsymbol{\\mu}_w = & \\mathbf{C}_w \\sigma^{-2} \\boldsymbol{\\Phi}^\\top \\mathbf{y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularised Mean\n",
    "\n",
    "Bayesian solution of the mean:\n",
    "$$\n",
    "  \\boldsymbol{\\mu}_w = \\left( \\sigma^{-2} \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} + \\alpha^{-1}\\mathbf{I} \\right)^{-1} \\sigma^{-2} \\boldsymbol{\\Phi}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "(cf) In ** week 3** we derived $\\mathbf{w}^* = (\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi})^{-1} \\boldsymbol{\\Phi}^\\top \\mathbf{y}$ .\n",
    "\n",
    "- two are equivalent when $\\alpha \\rightarrow \\infty$, ie, $\\boldsymbol{\\mu}_w$ is equivalent to  $\\mathbf{w}^*$ when the variance is infinite\n",
    "- in other cases $\\alpha \\mathbf{I}$ regularises the system, ie, keeps parameters smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Marginal Likelihood\n",
    "\n",
    "The marginal likelihood is the probability $p(\\mathbf{y} | \\boldsymbol{\\Phi},\\sigma^2)$ where $\\mathbf{w}$ has been integrated out:\n",
    "$$\n",
    "  p(\\mathbf{y} | \\boldsymbol{\\Phi},\\sigma^2) = \\int p(\\mathbf{y} | \\boldsymbol{\\Phi},\\mathbf{w},\\sigma^2) p(\\mathbf{w}) d\\mathbf{w}\n",
    "$$\n",
    "\n",
    "(note) by denoting $\\mathbf{K} = \\alpha \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^\\top + \\sigma^2 \\mathbf{I}$, further calculation leads to:\n",
    "$$\n",
    "  p(\\mathbf{y} | \\boldsymbol{\\Phi},\\sigma^2) = \\frac{1}{(2\\pi)^\\frac{n}{2} |\\mathbf{K}|^\\frac{1}{2}} \\exp\\left( -\\frac{1}{2} \\mathbf{y}^\\top \\mathbf{K}^{-1} \\mathbf{y} \\right)\n",
    "$$\n",
    "which is a zero mean $n$-dimensional Gaussian with covariance $\\mathbf{K}$ .\n",
    "\n",
    "Wikipedia: [Marginal Likelihood](https://en.wikipedia.org/wiki/Marginal_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making Prediction: Expected Output\n",
    "\n",
    "Given the posterior for the parameters, how can we compute the expected output at a given location?\n",
    "\n",
    "- output of the model at location $x_i$ is given by $f(x_i) = \\boldsymbol{\\phi}_i^\\top \\mathbf{w}$\n",
    "- we want the expected output under the posterior density, $p(\\mathbf{w} | \\boldsymbol{\\Phi},\\mathbf{y},\\sigma^2)$\n",
    "\n",
    "we now calculate the expected output:\n",
    "$$\n",
    "  \\mathbb{E}\\left[ f(x_i) \\right] = \\int f(x_i) p(\\mathbf{w} | \\boldsymbol{\\Phi},\\mathbf{y},\\sigma^2) d\\mathbf{w} = \\boldsymbol{\\phi}_i^\\top \\underbrace{\\int \\mathbf{w} \\cdot p(\\mathbf{w} | \\boldsymbol{\\Phi},\\mathbf{y},\\sigma^2) d\\mathbf{w}}_{\\boldsymbol{\\mu}_w} = \\boldsymbol{\\phi}_i^\\top \\boldsymbol{\\mu}_w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making Prediction: Variance of the Expected Output\n",
    "\n",
    "Variance of the model at location $x_i$ is given by\n",
    "\\begin{align*}\n",
    "  var\\left[ f(x_i) \\right]\n",
    "  & = \\mathbb{E}\\left[ f(x_i)^2 \\right] - \\left( \\mathbb{E}\\left[ f(x_i) \\right] \\right)^2 \\\\\n",
    "  & = \\boldsymbol{\\phi}_i^\\top \\mathbb{E}\\left[ \\mathbf{w} \\mathbf{w}^\\top \\right] \\boldsymbol{\\phi}_i - \\boldsymbol{\\phi}_i^\\top \\mathbb{E}\\left[ \\mathbf{w} \\right] \\mathbb{E}\\left[ \\mathbf{w} \\right]^\\top \\boldsymbol{\\phi}_i \\\\\n",
    "  & = \\boldsymbol{\\phi}_i^\\top \\underbrace{\\left( \\mathbb{E}\\left[ \\mathbf{w} \\mathbf{w}^\\top \\right] - \\mathbb{E}\\left[ \\mathbf{w} \\right] \\mathbb{E}\\left[ \\mathbf{w} \\right]^\\top \\right)}_{\\mathbf{C}_w} \\boldsymbol{\\phi}_i \\\\\n",
    "  & = \\boldsymbol{\\phi}_i^\\top \\mathbf{C}_w \\boldsymbol{\\phi}_i\n",
    "\\end{align*}\n",
    "where all these expectations are calculated under the posterior density, $p(\\mathbf{w} | \\boldsymbol{\\Phi},\\mathbf{y},\\sigma^2)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Estimates: Olympic Marathon Data\n",
    "\n",
    "Here we apply a Bayesian technique on Olympic data using a regression model, $\\mathbf{y} = \\boldsymbol{\\Phi}\\mathbf{w} + \\boldsymbol{\\epsilon}$, with\n",
    "\\begin{align*}\n",
    "  \\boldsymbol{\\epsilon} & \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I}) \\\\\n",
    "  \\mathbf{w} & \\sim \\mathcal{N}(\\mathbf{0}, \\alpha\\mathbf{I})\n",
    "\\end{align*}\n",
    "where $\\sigma^2 = 0.01$ and $\\alpha=1$ .\n",
    "\n",
    "(next graph) finding Bayesian polynomial fits with the number of basis between 1 and 27;\n",
    "solid line shows the expected output, $\\boldsymbol{\\phi}_i^\\top \\boldsymbol{\\mu}_w$, while dashed lines indicate the width for one standard diviation on the both side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./diagrams/olympic_BLM_polynomial_num_basis027.svg'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_plots('olympic_BLM_polynomial_num_basis{num_basis:0>3}.svg', directory='./diagrams', num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Estimates: Hold Out Validation\n",
    "\n",
    "(next graph) suppose we know the winning time (red circles) up to year 1984, are we able to predict those (green) for the rest of years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./diagrams/olympic_val_BLM_polynomial_num_basis027.svg'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_plots('olympic_val_BLM_polynomial_num_basis{num_basis:0>3}.svg', directory='./diagrams', num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Estimates: 5-fold Cross Validation\n",
    "\n",
    "(next graph) 5-fold cross validation: validation data is in green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./diagrams/olympic_5cv03_BLM_polynomial_num_basis027.svg'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_plots('olympic_5cv{part:0>2}_BLM_polynomial_num_basis{num_basis:0>3}.svg', directory='./diagrams', part=(0, 5), num_basis=(1, max_basis))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
