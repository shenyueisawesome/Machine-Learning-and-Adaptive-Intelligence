{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### week 8\n",
    "\n",
    "# Constrained Optimisation and Maximum Entropy Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uncertainty\n",
    "\n",
    "Here we consider a function $H$ that expresses **uncertainty**. $H$ should satisfy the following set of requirements:\n",
    "\n",
    "1. $H(p_1, \\ldots, p_n)$ is a maximum when $\\displaystyle p_1 = \\ldots = p_n = \\frac{1}{n}$.\n",
    "\n",
    "2. $H$ is a symmetric function of the arguments $p_1, \\ldots, p_n$. It means that\n",
    "$$\n",
    "  H(p_1, \\ldots, p_n) = H(p_{\\pi(1)}, \\ldots, p_{\\pi(n)})\n",
    "$$\n",
    "for any permutation $\\pi$ of $(1, \\ldots, n)$. In other words, only the probabilities matter, not their order.\n",
    "\n",
    "3. $H(p_1, \\ldots, p_n) \\geq 0$ and the equality holds only when one of $p_i$ is $1$. It means that uncertainty is inherently a positive quantity, and it is zero only when there is no randomness present.\n",
    "      \n",
    "4. $$\n",
    "  H\\left(\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right) \\leq H\\left(\\frac{1}{n + 1}, \\ldots, \\frac{1}{n + 1}\\right)\n",
    "$$\n",
    "(interpretation) Two-horse race is less uncertain than a three-horse race.\n",
    "\n",
    "5. $$\n",
    "  H(p_1, \\ldots, p_n, 0) = H(p_1, \\ldots, p_n)\n",
    "$$\n",
    "(interpretation) Uncertainty of a six-sided die is the same as a seven-sided die that has no chance of showing 7 but otherwise fair.\n",
    "\n",
    "6. $H(p_1, \\ldots, p_n)$ is a continuous function of its arguments. It means that a small change in the probabilities should not drastically affects the uncertainty.\n",
    "\n",
    "7. If $m$ and $n$ are positive integers,\n",
    "$$\n",
    "  H\\left(\\frac{1}{mn}, \\ldots, \\frac{1}{mn}\\right) = H\\left(\\frac{1}{m}, \\ldots, \\frac{1}{m}\\right) + H\\left(\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right)\n",
    "$$\n",
    "(interpretation) Uncertainty for throwing an $m$-sided die followed by an $n$-sided die should be the sum of individual uncertainties. This is a **linearity condition**.\n",
    "\n",
    "8. Let $p = p_1 + \\ldots + p_m$ and $q = q_1 + \\ldots + q_n$ where each $p_i$ and $q_j$ are non-negative. If $p$ and $q$ are positive and $p + q = 1$,\n",
    "$$\n",
    "  H(p_1, \\ldots, p_m, q_1, \\ldots, q_n) = H(p, q) + p H\\left(\\frac{p_1}{p}, \\ldots, \\frac{p_m}{p}\\right) + q H\\left(\\frac{q_1}{q}, \\ldots, \\frac{q_n}{q}\\right)\n",
    "$$\n",
    "(interpretation) Think about a race in which there are $m$ brown horses and $n$ grey horses with $p_i$ (or $q_j$) being the probability the $i^{th}$ brown horse (or $j^{th}$ grey horse) wins. The total uncertainty in the outcome is the uncertainty associated with a brown or grey winner plus the weighted sum of the uncertainties given that the winner is respectively brown or grey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "Entropy $H$ is defined as\n",
    "$$\n",
    "  H = \\sum_{i = 1 \\ldots m} p_i \\log_b \\frac{1}{p_i} = - \\sum_{i = 1 \\ldots m} p_i \\log_b p_i\n",
    "$$\n",
    "\n",
    "- Typical choices for the base $b$ are $2$, $e$, $10$.\n",
    "\n",
    "- Suppose the log is taken to the base $2$ (ie, $b=2$), entropy is expressed in *bits*.\n",
    "\n",
    "- In this notebook, we drop $b$ and use **natural log** with the base $e$, unless otherwise noted.\n",
    "\n",
    "- The convention is that $\\displaystyle 0\\log \\frac{1}{0} = 0$, because $\\displaystyle \\lim_{a\\rightarrow 0+} a\\log \\frac{1}{a} \\rightarrow 0$.\n",
    "\n",
    "- $H$ satisfies the requirements 1. to 8. above for measuring the **uncertainty**.\n",
    "\n",
    "- Alternatively, the entropy of a random variable $X$ may be written as\n",
    "$$\n",
    "  H(X) = \\sum_{x\\in {\\cal X}} p(x) \\log \\frac{1}{p(x)}\n",
    "$$\n",
    "\n",
    "- Depending on the context, we may write $H(p)$ for the above quantity.\n",
    "\n",
    "Here are some examples for calculating entropies with base $2$ and expressing the result in *bits*:\n",
    "\n",
    "1. A flip of a fair coin:\n",
    "$$\n",
    "  H = \\frac{1}{2}\\log_2 2 + \\frac{1}{2}\\log_2 2 = 1 \\ (\\text{bit})\n",
    "$$\n",
    "\n",
    "2. A flip of a biased coin, with the head coming up as twice frequently as the tail:\n",
    "$$\n",
    "  H = \\frac{2}{3}\\log_2\\frac{3}{2} + \\frac{1}{3}\\log_2 3 \\sim 0.92 \\ (\\text{bits})\n",
    "$$\n",
    "\n",
    "3. A race between four horses, having a chance to win with probabilities $0.4$, $0.3$, $0.2$, and $0.1$:\n",
    "$$\n",
    "  H = 0.4\\log_2 \\frac{1}{0.4} + 0.3\\log_2 \\frac{1}{0.3} + 0.2\\log_2 \\frac{1}{0.2} + 0.1\\log_2 \\frac{1}{0.1} \\sim 1.85 \\ (\\text{bits})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy: Lower Bound and Upper Bound\n",
    "\n",
    "We can prove that entropy is *not negative*.\n",
    "\n",
    "For a random variable $X$:\n",
    "$$\n",
    "  0 \\leq p(x) \\leq 1\n",
    "  \\quad \\Longrightarrow \\quad \\log \\frac{1}{p(x)} \\geq 0\n",
    "  \\quad \\Longrightarrow \\quad H(X) = \\sum_{x\\in {\\cal X}} p(x) \\log \\frac{1}{p(x)} \\geq 0\n",
    "$$\n",
    "Next, because $H(X) \\geq 0$, $H(X) = 0$ implies that\n",
    "$$\n",
    "  p(x) \\log \\frac{1}{p(x)} = 0 \\text{ for all } x\n",
    "  \\quad \\Longrightarrow \\quad p(x) = 0 \\text{ or } \\log \\frac{1}{p(x)} = 0 \\text{ for all } x\n",
    "  \\quad \\Longrightarrow \\quad p(x) = 0 \\text{ or } 1 \\text{ for all } x\n",
    "$$\n",
    "using our standard convention $\\displaystyle 0\\cdot\\log\\frac{1}{0} = 0$ because $\\displaystyle \\lim_{a\\rightarrow 0+} a\\cdot\\log\\frac{1}{a} \\rightarrow 0$.\n",
    "\n",
    "Thus $H(X) = 0$ when $p(x) = 1$ for one $x$ and $0$ for the rest. (Recall the probability axiom.)\n",
    "\n",
    "We can also prove that entropy has an upper bound, defined by the inequality, $H(p) \\leq \\log |{\\cal X}|$, which  is achieved when $p(x)$ is uniformly distributed over all $x\\in{\\cal X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More Entropies\n",
    "\n",
    "There are more extropies, such as *joint entropy*, *conditional entropy* and *mutual information*.\n",
    "\n",
    "#### Joint Entropy\n",
    "\n",
    "The joint entropy of $X$ and $Y$ is defined as\n",
    "$$\n",
    "  H(X,Y) = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} p(x,y) \\log \\frac{1}{p(x,y)}\n",
    "$$\n",
    "\n",
    "#### Conditional Entropy\n",
    "\n",
    "The conditional entropy of $Y$ given $X$ is defined as\n",
    "\\begin{align*}\n",
    "  H(Y|X)\n",
    "  &= \\sum_{x\\in {\\cal X}} p(x) H(Y|X = x) \\\\\n",
    "  &= \\sum_{x\\in {\\cal X}} p(x) \\sum_{y\\in {\\cal Y}} p(y|x) \\log \\frac{1}{p(y|x)} \\\\\n",
    "  &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} p(x,y) \\log \\frac{1}{p(y|x)}\n",
    "\\end{align*}\n",
    "\n",
    "#### Mutual information\n",
    "\n",
    "The mutual information between $X$ and $Y$ is defined as\n",
    "$$\n",
    "  I(X;Y) = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}\n",
    "$$\n",
    "This $I(X;Y)$ can be rewritten as\n",
    "$$\n",
    "  I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n",
    "$$\n",
    "It measures the average reduction in uncertainty about $X$ that results from learning the value of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Forecast\n",
    "\n",
    "How do we forecast tomorrow's weather?\n",
    "\n",
    "- satellite\n",
    "- history, or one's experience\n",
    "- coin\n",
    "\n",
    "How do we handle information from various sources?\n",
    "\n",
    "- **backoff:** choose one\n",
    "    1. Use satellite image by default.\n",
    "    2. If 1. is not available, rely on statistics (ie, past history).\n",
    "    3. If 2. is not available, flip a coin prediction.\n",
    "\n",
    "- **interpolation:** weight each evidence, eg, $0.6\\times\\text{satellite} + 0.3\\times\\text{history} + 0.1\\times\\text{coin}$\n",
    "\n",
    "- **maximum entropy:** make a least biased decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy\n",
    "\n",
    "#### Coin Tossing\n",
    "\n",
    "Suppose a weighted coin has probability $h$ of coming up heads,\tentropy of tossing the coin only once is given by\n",
    "$$\n",
    "  H(X) = h \\log_2 \\frac{1}{h} + (1 - h) \\log_2 \\frac{1}{1 - h}\n",
    "$$\n",
    "\n",
    "Entropy is maximised when the coin is fair, ie, unbiased:\n",
    "<img src=\"./figs/coin_toss.jpg\", width=300, align=center>\n",
    "\n",
    "#### Concept\n",
    "\n",
    "- model *all* that is known\n",
    "- assume *nothing* that is unknown\n",
    "\n",
    "#### Principle\n",
    "\n",
    "Given a collection of facts, the **maximum entropy** method choose a model that is *consistent with all facts*, but otherwise *as uniform as possible*.\n",
    "\n",
    "#### Simple Example\n",
    "\n",
    "We wish to estimate a joint probability distribution $p(x, y)$ where $x\\in\\{x_1, x_2\\}$ and $y\\in\\{y_1, y_2\\}$, given the constraints\n",
    "\\begin{align*}\n",
    "  p(x_1, y_1) + p(x_2, y_1) &= 0.6 \\\\\n",
    "  p(x_1, y_1) + p(x_1, y_2) + p(x_2, y_1) + p(x_2, y_2) &= 1\n",
    "\\end{align*}\n",
    "Given these constraints, our objective is to maximise\n",
    "$$\n",
    "  H(X, Y) = \\sum_{x\\in\\{x_1, x_2\\}} \\sum_{y\\in\\{y_1, y_2\\}} p(x, y) \\log\\frac{1}{p(x, y)}\n",
    "$$\n",
    "\n",
    "One distribution that satisfies constrains:\n",
    "\n",
    ".     | $y_1$ | $y_2$ |\n",
    ":----:|:-----:|:-----:|:-----:\n",
    "$x_1$ | 0.5   | 0.1   |\n",
    "$x_2$ | 0.1   | 0.3   |\n",
    "total | 0.6   |       | 1.0\n",
    "\n",
    "The most uniform distribution that satisfies constrains:\n",
    "\n",
    ".     | $y_1$ | $y_2$ |\n",
    ":----:|:-----:|:-----:|:-----:\n",
    "$x_1$ | 0.3   | 0.2   |\n",
    "$x_2$ | 0.3   | 0.2   |\n",
    "total | 0.6   |       | 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Forecast: Analytical Solution Using Lagrange Multiplier\n",
    "\n",
    "#### Problem\n",
    "\n",
    "Firstly, we make overly simplified assumption that five weather types, \\{misty, foggy, cloudy, sunny, rainy\\}, can fully describe weather. Now, suppose that today's weather is \\{cloudy\\}, what will be tomorrow's weather?\n",
    "\n",
    "#### A Single Constraint\n",
    "\n",
    "Initially we have the *total probability* constraint:\n",
    "$$\n",
    "  p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) = 1\n",
    "$$\n",
    "There exist infinite combinations of probabilities that may satisfy the constraint above.\n",
    "\n",
    "The most intuitively appealing model is\n",
    "\\begin{align*}\n",
    "  p(misty) &= 0.2 \\\\\n",
    "  p(foggy) &= 0.2 \\\\\n",
    "  p(cloudy) &= 0.2 \\\\\n",
    "  p(sunny) &= 0.2 \\\\\n",
    "  p(rainy) &= 0.2\n",
    "\\end{align*}\n",
    "This model allocates the total probability (that is ` 1 ') evenly among five possible weathers. It is the most *uniform* model subject to our knowledge... but what is exactly meant by *uniform*?\n",
    "\n",
    "Analytically, we wish to maximise the entropy $H(Y)$ given the total probability constraint. Using a Lagrange multiplier $\\lambda_1$:\n",
    "$$\n",
    "  \\Lambda = H(Y) + \\lambda_1 \\times \\text{constraint}\n",
    "  = \\left( - \\sum_{y\\in {\\cal Y}} p(y) \\log p(y) \\right) + \\lambda_1 \\left( \\sum_{y\\in {\\cal Y}} p(y) - 1 \\right)\n",
    "$$\n",
    "and take a partial derivative with respect to $p(y)$:\n",
    "$$\n",
    "  \\frac{\\partial\\Lambda}{\\partial p(y)} = - \\log p(y) - 1 + \\lambda_1\n",
    "$$\n",
    "Then set $\\displaystyle \\frac{\\partial\\Lambda}{\\partial p(y)} = 0$ and, finally, get $p(y) = 0.2$ for all $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 1\n",
    "\n",
    "Solve $\\displaystyle \\frac{\\partial\\Lambda}{\\partial p(y)} = 0$ for the weather forecast problem with *a single* constraint to reach the most *uniform* model that is presented above. Show your working fully.\n",
    "\n",
    "*5 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Two Constraints\n",
    "\n",
    "Suppose we have *two* constraints:\n",
    "\\begin{align*}\n",
    "  p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) &= 1 \\\\\n",
    "  p(misty) + p(foggy) &= 0.3\n",
    "\\end{align*}\n",
    "By observation, the most *uniform* model is\n",
    "\\begin{align*}\n",
    "  p(misty) &= 0.15 \\\\\n",
    "  p(foggy) &= 0.15 \\\\\n",
    "  p(cloudy) &= 0.233... \\\\\n",
    "  p(sunny) &= 0.233... \\\\\n",
    "  p(rainy) &= 0.233...\n",
    "\\end{align*}\n",
    "\n",
    "Analytically, we maximise $H(Y)$ given those two constraints:\n",
    "\\begin{align*}\n",
    "  \\Lambda = H(Y)\n",
    "    & + \\lambda_1 \\{ p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) - 1 \\} \\\\\n",
    "    & + \\lambda_2 \\{ p(misty) + p(foggy) - 0.3 \\}\n",
    "\\end{align*}\n",
    "and calculate partial derivatives:\n",
    "$$\n",
    "  \\frac{\\partial\\Lambda}{\\partial p(y)} = \\left\\{ \\begin{array}{ll}\n",
    "    - \\log p(y) - 1 + \\lambda_1 + \\lambda_2\t& y = misty, foggy \\\\\n",
    "    - \\log p(y) - 1 + \\lambda_1\t\t& \\text{otherwise}\n",
    "    \\end{array} \\right.\n",
    "$$\n",
    "Then set $\\displaystyle \\frac{\\partial\\Lambda}{\\partial p(y)} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 2\n",
    "\n",
    "Solve $\\displaystyle \\frac{\\partial\\Lambda}{\\partial p(y)} = 0$ for the weather forecast problem with *two* constraint to reach the most *uniform* model that is presented above. Show your working fully.\n",
    "\n",
    "*5 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Three Constraints\n",
    "\n",
    "Suppose we have *three* constraints:\n",
    "\\begin{align*}\n",
    "  p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) &= 1 \\\\\n",
    "  p(misty) + p(foggy) &= 0.3 \\\\\n",
    "  p(misty) + p(cloudy) &= 0.5\n",
    "\\end{align*}\n",
    "\n",
    "Solution is no longer obvious, but we still can work on this case analytically.\n",
    "\n",
    "We maximise $H(Y)$ given three constraints:\n",
    "\\begin{align*}\n",
    "  \\Lambda = H(Y)\n",
    "    & + \\lambda_1 \\{ p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) - 1 \\} \\\\\n",
    "    & + \\lambda_2 \\{ p(misty) + p(foggy) - 0.3 \\} \\\\\n",
    "    & + \\lambda_3 \\{ p(misty) + p(cloudy) - 0.5 \\}\n",
    "\\end{align*}\n",
    "Thus,\n",
    "$$\n",
    "  \\frac{\\partial\\Lambda}{\\partial p(y)} = \\left\\{ \\begin{array}{ll}\n",
    "    - \\log p(y) - 1 + \\lambda_1 + \\lambda_2 + \\lambda_3 & y = misty \\\\\n",
    "    - \\log p(y) - 1 + \\lambda_1 + \\lambda_2\t\t& y = foggy \\\\\n",
    "    - \\log p(y) - 1 + \\lambda_1 + \\lambda_3\t\t& y = cloudy \\\\\n",
    "    - \\log p(y) - 1 + \\lambda_1\t\t\t& \\text{otherwise}\n",
    "\t\\end{array} \\right.\n",
    "$$\n",
    "We set $\\displaystyle \\frac{\\partial\\Lambda}{\\partial p(y)} = 0$ and get the most *uniform* model:\n",
    "\\begin{align*}\n",
    "  p(misty) &= \\frac{9 - \\sqrt{51}}{10} = 0.186... \\\\\n",
    "  p(foggy) &= \\frac{\\sqrt{51} - 6}{10} = 0.114... \\\\\n",
    "  p(cloudy) &= \\frac{\\sqrt{51} - 4}{10} = 0.314... \\\\\n",
    "  p(sunny) &= \\frac{11 - \\sqrt{51}}{20} = 0.193... \\\\\n",
    "  p(rainy) &= \\frac{11 - \\sqrt{51}}{20} = 0.193...\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 3\n",
    "\n",
    "Solve $\\displaystyle \\frac{\\partial\\Lambda}{\\partial p(y)} = 0$ for the weather forecast problem with *three* constraint to reach the most *uniform* model that is presented above. Show your working fully.\n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model\n",
    "\n",
    "#### Random Process\n",
    "\n",
    "Formally, we define a random process as follows:\n",
    "\\begin{align*}\n",
    "  x &: \\text{some information influencing the output}, \\ x\\in{\\cal X} \\\\\n",
    "  y &: \\text{output value}, \\ y\\in{\\cal Y}\n",
    "\\end{align*}\n",
    "\n",
    "As an example, a random process can be defined for the weather forecast problem:\n",
    "\\begin{align*}\n",
    "  x &: \\text{today's weather}, \\ x\\in\\{cloudy\\} \\\\\n",
    "  y &: \\text{tomorrow's weather}, \\ y\\in\\{misty, foggy, cloudy, sunny, rainy\\}\n",
    "\\end{align*}\n",
    "\n",
    "#### Training Samples\n",
    "\n",
    "We also have training samples $(x_1, y_1), (x_2, y_2), \\ \\ldots \\ , (x_N, y_N)$.\n",
    "\n",
    "We assume that there are ten training samples for the weather forecast problem:\n",
    "\\begin{align*}\n",
    "  & (cloudy, cloudy), (cloudy, sunny), (cloudy, sunny), (cloudy, misty), (cloudy, cloudy), \\\\\n",
    "  & (cloudy, rainy), (cloudy, misty), (cloudy, foggy), (cloudy, cloudy), (cloudy, rainy)\n",
    "\\end{align*}\n",
    "\n",
    "#### Feature\n",
    "\n",
    "For $i = 1,\\ldots,n$ ($n$: number of features), we define $f_i(x, y)$, an indicator function of type ${\\cal X}\\times{\\cal Y}\\longrightarrow\\{0, 1\\}$.\n",
    "\n",
    "The weather forecast problem has the following feature set:\n",
    "\\begin{align*}\n",
    "  f_1(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, foggy, cloudy, sunny, rainy\\} \\\\\n",
    "  f_2(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, foggy\\} \\\\\n",
    "  f_3(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, cloudy\\}\n",
    "\\end{align*}\n",
    "otherwise $f_i(x, y) = 0$ for $i = 1,2,3$.\n",
    "\n",
    "- Note that $x$ is always $\\{cloudy\\}$ with this problem.\n",
    "- If one sample is $(cloudy, foggy)$: $f_1 = 1, f_2 = 1, f_3 = 0$.\n",
    "\n",
    "#### Expected Values\n",
    "\n",
    "The expected value of $f_i$ with respect to an *empirical distribution* $\\tilde{p}(x, y)$ is given by\n",
    "$$\n",
    "  \\tilde{p}(f_i) \\equiv E_{\\tilde{p}}[f_i] = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x, y) f_i(x, y)\n",
    "$$\n",
    "\n",
    "- $\\tilde{p}(x, y)$ represents a summary of the training sample, that is,\n",
    "$$\n",
    "  \\displaystyle \\tilde{p}(x, y) \\equiv \\frac{1}{N} \\times \\text{number of times that $(x, y)$ occurs in the sample}\n",
    "$$\n",
    "- Some pair $(x, y)$ may not occur at all in the sample.\n",
    "\n",
    "The expected value of $f_i$ with respect to a *model distribution* $p(x, y)$ is given by\n",
    "$$\n",
    "  p(f_i) \\equiv E_p[f_i] = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} p(x, y) f_i(x, y) \\sim \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p(y | x) f_i(x, y)\n",
    "$$\n",
    "\n",
    "- Calculation of $p(f_i)$ with respect to $p(x, y)$ is to the order of $|\\ {\\cal X}\\times{\\cal Y}\\ |$, which is often too large.\n",
    "- Instead, by using the empirical distribution $\\tilde{p}(x)$, the calculation gets more tractable because we only consider those in the training sample.\n",
    "- We are likely to have more reliable estimates for $p(y | x)$ than for $p(x, y)$.\n",
    "\n",
    "Now we have the following **constraint** that relates two expected values:\n",
    "$$\n",
    "  p(f_i) = \\tilde{p}(f_i)\n",
    "$$\n",
    "where $\\tilde{p}(f_i)$ is a mean of representing statistical phenomena in the training sample, and $p(f_i) = \\tilde{p}(f_i)$ is a mean of requiring that our model generalises these phenomena.\n",
    "\n",
    "Here we can write down constraints for the weather forecast problem:\n",
    "\\begin{align*}\n",
    "  p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) &= 1 \\\\\n",
    "  p(misty) + p(foggy) &= 0.3 \\\\\n",
    "  p(misty) + p(cloudy) &= 0.5\n",
    "\\end{align*}\n",
    "\n",
    "#### Maximum Entropy Model\n",
    "\n",
    "Of all conditional probability distributions ${\\cal P}$, a subset ${\\cal C} \\equiv \\{ \\ p\\in{\\cal P} \\ | \\ p(f_i) = \\tilde{p}(f_i) \\ \\text{ for } \\ i = 1,\\ldots,n \\ \\}$ constrains the model according to our knowledge.\n",
    "The following conditional entropy indicates a *mathematical measure of the uniformity*:\n",
    "$$\n",
    "  H_p(Y|X) \\equiv - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p(y | x) \\log p(y | x)\n",
    "$$\n",
    "where notation $H_p(Y|X)$ emphasises the dependency of the entropy on $p$.\n",
    "\n",
    "Finally, we have reached the maximum entropy model of the form:\n",
    "$$\n",
    "  p^* = \\mathop{\\rm argmax}_{p\\in{\\cal C}} H_p(Y|X)\n",
    "$$\n",
    "\n",
    "#### Parameter estimation procedure\n",
    "\n",
    "For each feature $f_i \\ (i = 1,\\ldots,n)$, we introduce a *Lagrange multiplier* $\\lambda_i$, then define the *Lagrangian*:\n",
    "$$\n",
    "  \\Lambda(p, \\lambda) \\equiv H_p(Y|X) + \\sum_i \\lambda_i \\{ p(f_i) - \\tilde{p}(f_i) \\}\n",
    "$$\n",
    "    \n",
    "Holding $\\lambda = \\{\\lambda_i\\}$ fixed, we compute the unconstrained maximum of $\\Lambda(p, \\lambda)$ over all $p\\in{\\cal P}$ :\n",
    "\\begin{align*}\n",
    "  p_{\\lambda}(y|x) &= \\mathop{\\rm argmax}_{p\\in{\\cal P}} \\Lambda(p, \\lambda) \\\\\n",
    "  \\Phi(\\lambda) &= \\Lambda(p_{\\lambda}, \\lambda)\n",
    "\\end{align*}\n",
    "\n",
    "General solution is the exponential model:\n",
    "\\begin{align*}\n",
    "  p_{\\lambda}(y|x) &= \\frac{1}{Z_{\\lambda}(x)} \\exp \\left( \\sum_i \\lambda_i f_i(x,y) \\right) \\\\\n",
    "  \\Phi(\\lambda) &= - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\log Z_{\\lambda}(x) + \\sum_i \\lambda_i \\tilde{p}(f_i)\n",
    "\\end{align*}\n",
    "where $Z_{\\lambda}(x)$ is a normalising constant given by\n",
    "$$\n",
    "  Z_{\\lambda}(x) = \\sum_{y\\in {\\cal Y}} \\exp \\left( \\sum_i \\lambda_i f_i(x,y) \\right)\n",
    "$$\n",
    "Technically, $\\lambda_i$ is a *Lagrange multiplier*, associated with the feature $f_i(x,y)$, in a certain constrained optimisation problem. In a sense, $\\lambda_i$ is a measure of the *importance* of the feature $f_i(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 4\n",
    "\n",
    "A box contains an unknown number of balls. Each ball is labelled with an integer between $1$ and $N$. There may be *any number of balls* (ie, zero, one, or more than one ball) having the same label. For each of the following two cases, you are asked to define appropriate variables and to write a Lagrangian. Note that you do **not** need to solve Lagrangians.\n",
    "\n",
    "**Task 1:**\n",
    "There is only one box in a room. You draw one ball each time, record the label, then return it to the box. After repeating the above procedure several times, you have found that the average value of the labels was $L$. Write a Lagrangian that leads to a solution for the most unbiased estimate of the number of different labels.\n",
    "\n",
    "**Task 2:**\n",
    "This time there are many boxes in a room. It is known that, on average, the total number of balls in one box is $T$, and the sum of labels in one box is $S$. Write a Lagrangian that leads to a solution for the most unbiased distribution of the number of different labels.\n",
    "\n",
    "*20 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Qustion 4 Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Function\n",
    "\n",
    "An auxiliary function is a (pointwise) lower or upper bound on a function.\n",
    "<img src=\"./figs/auxiliary.jpg\", width=300, align=center>\n",
    "\n",
    "$x-1$ is an auxiliary function for $\\log x$, and $\\log x$ is an auxiliary function for $x-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model: Numerical Solution\n",
    "\n",
    "We can formulate the task (ie, that of finding a solution to the maximum entropy model) as a pure *maximum likelihood* problem of discovering the optimal values for the model parameters.\n",
    "\n",
    "We start with the log likelihood of the general solution $p_{\\lambda}(y|x)$:\n",
    "\\begin{align*}\n",
    "  {\\cal L}(\\tilde{p} | p_{\\lambda}) &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\log p_{\\lambda}(y|x) \\\\\n",
    "  &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\sum_i \\lambda_i f_i(x,y) - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\log Z_{\\lambda}(x) \\underbrace{\\sum_{y\\in {\\cal Y}} \\tilde{p}(y|x)}_1\n",
    "\\end{align*}\n",
    "\n",
    "Differentiating ${\\cal L}(\\tilde{p} | p_{\\lambda})$ with respect to an individual $\\lambda_i$,\n",
    "\\begin{align*}\n",
    "  \\frac{\\partial {\\cal L}(\\tilde{p} | p_{\\lambda})}{\\partial \\lambda_i}\n",
    "  &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\frac{\\partial}{\\partial \\lambda_i} \\sum_i \\lambda_i f_i(x,y) - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\frac{\\partial \\log Z_{\\lambda}(x)}{\\partial \\lambda_i} \\\\\n",
    "  &= \\underbrace{\\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) f_i(x,y)}_{\\tilde{p}(f_i)} - \\underbrace{\\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) f_i(x,y)}_{p(f_i)}\n",
    "\\end{align*}\n",
    "\n",
    "Setting $\\displaystyle \\frac{\\partial {\\cal L}(\\tilde{p} | p_{\\lambda})}{\\partial \\lambda_i}$ to zero yields the condition for maximising the log likelihood with respect to $\\lambda_i$.\n",
    "In other words, the maximum entropy model $p*\\in{\\cal C}$ also maximises the *likelihood* of the training sample $\\tilde{p}$ .\n",
    "\n",
    "A **numerical solution** is the procedure that iteratively achieves\n",
    "$$\n",
    "  \\lambda \\equiv \\{ \\lambda_1, \\ldots, \\lambda_n \\}\n",
    "  \\quad\\longrightarrow\\quad \\lambda + \\delta \\equiv\n",
    "  \\{ \\lambda_1 + \\delta_1, \\ldots, \\lambda_n + \\delta_n \\}\n",
    "$$\n",
    "where $\\lambda + \\delta$ is not inferior to $\\lambda$ in a *log likelihood* sense.\n",
    "\n",
    "To achieve this, we calculate\n",
    "\\begin{align*}\n",
    "  {\\cal L}(\\tilde{p} | p_{\\lambda + \\delta}) - {\\cal L}(\\tilde{p} | p_{\\lambda})\n",
    "  &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\log p_{\\lambda + \\delta}(y|x) - \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\log p_{\\lambda}(y|x) \\\\\n",
    "  &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\sum_i \\delta_i f_i(x,y) - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\log \\frac{Z_{\\lambda + \\delta}(x)}{Z_{\\lambda}(x)}\n",
    "\\end{align*}\n",
    "\n",
    "Using the inequality $-\\log a \\geq 1 - a$ for all $a > 0$, we establish a lower bound on the change in the log likelihood:\n",
    "$$\n",
    "  {\\cal L}(\\tilde{p} | p_{\\lambda + \\delta}) - {\\cal L}(\\tilde{p} | p_{\\lambda}) \\geq {\\cal A}(\\delta | \\lambda)\n",
    "$$\n",
    "where\n",
    "\\begin{align*}\n",
    "  {\\cal A}(\\delta | \\lambda)\n",
    "  = & \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\sum_i \\delta_i f_i(x,y) + 1 - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\frac{Z_{\\lambda + \\delta}(x)}{Z_{\\lambda}(x)} \\\\\n",
    "  = & \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\sum_i \\delta_i f_i(x,y) + 1 - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) \\exp \\left( \\sum_i \\delta_i f_i(x,y) \\right)\n",
    "\\end{align*}\n",
    "If we find $\\delta$ for which ${\\cal A}(\\delta | \\lambda) > 0$, then $\\lambda + \\delta$ is an improvement over $\\lambda$.\n",
    "\n",
    "**problem:**\n",
    "the straightforward maximisation of ${\\cal A}(\\delta | \\lambda)$ may not work, because differentiating ${\\cal A}(\\delta | \\lambda)$ with respect to $\\delta_i$ yields an equation containing $\\{ \\delta_1, \\ldots, \\delta_n \\}$ (ie, $\\delta_i$ is coupled).\n",
    "\n",
    "**solution:**\n",
    "to get around above problem, we introduce the quantity: $\\displaystyle f^{\\#}(x,y) \\equiv \\sum_i f_i(x,y)$, ie, the number of features that are not zero at $(x,y)$, and rewrite:\n",
    "$$\n",
    "  \\exp \\left( \\sum_i \\delta_i f_i(x,y) \\right) = \\exp \\left( \\sum_i \\frac{f_i(x,y)}{f^{\\#}(x,y)} \\delta_i f^{\\#}(x,y) \\right)\n",
    "$$\n",
    "\n",
    "Notice that $\\displaystyle \\frac{f_i(x,y)}{f^{\\#}(x,y)}$ is a pdf over $i$, that is non-negative and sums to one.\n",
    "\n",
    "For a convex function, $f(x) = e^x$, *Jensen's inequality* says\n",
    "\\begin{align*}\n",
    "  f(E[i]) &\\leq E[f(i)] \\\\\n",
    "  \\exp \\left( \\sum_i p(i) q(i) \\right) &\\leq \\sum_i p(i) \\exp q(i) \\\\\n",
    "  \\exp \\left( \\sum_i \\frac{f_i(x,y)}{f^{\\#}(x,y)} \\delta_i f^{\\#}(x,y)\\right) &\\leq \\sum_i \\frac{f_i(x,y)}{f^{\\#}(x,y)} \\exp \\left( \\delta_i f^{\\#}(x,y) \\right)\n",
    "\\end{align*}\n",
    "which results in ${\\cal A}(\\delta | \\lambda) \\geq {\\cal B}(\\delta | \\lambda)$ where\n",
    "$$\n",
    "  {\\cal B}(\\delta | \\lambda) = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\sum_i \\delta_i f_i(x,y) + 1 - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) \\sum_i \\frac{f_i(x,y)}{f^{\\#}(x,y)} \\exp \\left( \\delta_i f^{\\#}(x,y) \\right)\n",
    "$$\n",
    "\n",
    "${\\cal B}(\\delta | \\lambda)$ is a new, not as tight, lower bound on the change in log likelihood:\n",
    "$$\n",
    "  {\\cal L}(\\tilde{p} | p_{\\lambda + \\delta}) - {\\cal L}(\\tilde{p} | p_{\\lambda}) \\geq {\\cal B}(\\delta | \\lambda)\n",
    "$$\n",
    "\n",
    "Differentiating ${\\cal B}(\\delta | \\lambda)$ with respect to $\\delta_i$,\n",
    "$$\n",
    "  \\frac{\\partial {\\cal B}(\\delta | \\lambda)}{\\partial \\delta_i} = \\overbrace{\\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) f_i(x,y)}^{\\tilde{p}(f_i)} - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) f_i(x,y) \\exp \\left( \\delta_i f^{\\#}(x,y) \\right)\n",
    "$$\n",
    "\n",
    "What is nice about this? $\\quad\\Longrightarrow\\quad$ $\\delta_i$ appears alone without any coupling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Solution using the Iterative scaling algorithm (ISA)\n",
    "\n",
    "1. initially set $\\lambda_i = 0$ for all $i = 1,\\ldots,n$\n",
    "\n",
    "2. do for each $i = 1,\\ldots,n$ :\n",
    "\n",
    "   **A.** let $\\delta_i$ be the solution to\n",
    "   $$\n",
    "     \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) f_i(x,y) \\exp\\left( \\delta_i f^{\\#}(x,y) \\right) = \\tilde{p}(f_i)\n",
    "   $$\n",
    "   where $\\displaystyle f^{\\#}(x,y) \\equiv \\sum_{i = 1,\\ldots,n} f_i(x,y)$\n",
    "\n",
    "   **B.** update multipliers by $\\lambda_i \\leftarrow \\lambda_i + \\delta_i$\n",
    "\n",
    "3. go back to step 2, if not all $\\lambda_i$ have converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Forecast Problem (Revisited)\n",
    "\n",
    "Recall that, for $i = 1,\\ldots,n$ ($n$: number of features), we define $f_i(x, y)$, an indicator function of type ${\\cal X}\\times{\\cal Y}\\longrightarrow\\{0, 1\\}$.\n",
    "\n",
    "The weather forecast problem has the following feature set:\n",
    "\\begin{align*}\n",
    "  f_1(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, foggy, cloudy, sunny, rainy\\} \\\\\n",
    "  f_2(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, foggy\\} \\\\\n",
    "  f_3(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, cloudy\\}\n",
    "\\end{align*}\n",
    "otherwise $f_i(x, y) = 0$ for $i = 1,2,3$.\n",
    "\n",
    "We now create a matix for the indicator functions, from which we want to count the number of *active features* by\n",
    "$$\n",
    "  f^{\\#}(x,y) \\equiv \\sum_{i = 1,\\ldots,n} f_i(x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of features\n",
    "num_feat = 5\n",
    "\n",
    "# indicator functions\n",
    "indicator = np.array([[1, 1, 1, 1, 1], [1, 1, 0, 0, 0], [1, 0, 1, 0, 0]])\n",
    "\n",
    "# counting the number of 'active features'\n",
    "active_feat = np.array([np.sum(indicator[:,0]), np.sum(indicator[:,1]), np.sum(indicator[:,2]), np.sum(indicator[:,3]), np.sum(indicator[:,4])])\n",
    "active_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers of active features for \\{misty, foggy, cloudy, sunny, rainy\\} are 3, 2, 2, 1, and 1, respectively.\n",
    "\n",
    "Next, we denote $a = e^{\\delta_1}$, $b = e^{\\delta_2}$ and $c = e^{\\delta_3}$, and define:\n",
    "\\begin{align*}\n",
    "  {\\cal G}(a) &= p(misty)\\cdot a^3 + p(foggy)\\cdot a^2 + p(cloudy)\\cdot a^2 + p(sunny)\\cdot a + p(rainy)\\cdot a - 1 \\\\\n",
    "  {\\cal G}(b) &= p(misty)\\cdot b^3 + p(foggy)\\cdot b^2 - 0.3 \\\\\n",
    "  {\\cal G}(c) &= p(misty)\\cdot c^3 + p(cloudy)\\cdot c^2 - 0.5\n",
    "\\end{align*}\n",
    "To obtain $a$, $b$, and $c$, we achieve the *Newton-Raphson*, eg, $\\displaystyle a_{n+1} = a_n - \\frac{{\\cal G} (a_n)}{{\\cal G}' (a_n)}$ for $a$.  The same for $b$ and $c$.\n",
    "\n",
    "\n",
    "We are able to calculate probabilities from multipliers $\\lambda_1$, $\\lambda_2$, and $\\lambda_3$ :\n",
    "\\begin{align*}\n",
    "  p(misty) &= \\frac{1}{Z_{\\lambda}} \\exp (\\lambda_1 + \\lambda_2 + \\lambda_3) \\\\\n",
    "  p(foggy) &= \\frac{1}{Z_{\\lambda}} \\exp (\\lambda_1 + \\lambda_2) \\\\\n",
    "  p(cloudy) &= \\frac{1}{Z_{\\lambda}} \\exp (\\lambda_1 + \\lambda_3) \\\\\n",
    "  p(sunny) &= \\frac{1}{Z_{\\lambda}} \\exp \\lambda_1 \\\\\n",
    "  p(rainy) &= \\frac{1}{Z_{\\lambda}} \\exp \\lambda_1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       initial prob: [[ 0.33333333  0.22222222  0.22222222  0.11111111  0.11111111]]\n",
      "iteration 1 :  prob: [[ 0.1678053   0.17475831  0.21325466  0.22209087  0.22209087]]\n",
      "iteration 2 :  prob: [[ 0.17323272  0.16150574  0.23223459  0.21651348  0.21651348]]\n",
      "iteration 3 :  prob: [[ 0.1769317   0.15136629  0.24776784  0.21196709  0.21196709]]\n",
      "iteration 4 :  prob: [[ 0.17946795  0.14354098  0.26041844  0.20828632  0.20828632]]\n",
      "iteration 5 :  prob: [[ 0.18122287  0.13745529  0.27069074  0.20531555  0.20531555]]\n",
      "iteration 6 :  prob: [[ 0.1824508   0.13269137  0.27901631  0.20292076  0.20292076]]\n",
      "iteration 7 :  prob: [[ 0.18332082  0.1289415   0.285756    0.20099084  0.20099084]]\n",
      "iteration 8 :  prob: [[ 0.1839455   0.12597622  0.2912076   0.19943534  0.19943534]]\n",
      "iteration 9 :  prob: [[ 0.18440015  0.12362243  0.29561501  0.19818121  0.19818121]]\n",
      "iteration 10 :  prob: [[ 0.18473552  0.12174815  0.29917697  0.19716968  0.19716968]]\n",
      "iteration 11 :  prob: [[ 0.18498611  0.12025186  0.30205498  0.19635353  0.19635353]]\n",
      "iteration 12 :  prob: [[ 0.1851756   0.11905481  0.30437999  0.1956948   0.1956948 ]]\n",
      "iteration 13 :  prob: [[ 0.18532047  0.1180955   0.30625805  0.19516299  0.19516299]]\n",
      "iteration 14 :  prob: [[ 0.1854323   0.11732563  0.30777497  0.19473355  0.19473355]]\n",
      "iteration 15 :  prob: [[ 0.18551936  0.11670707  0.30900012  0.19438672  0.19438672]]\n",
      "iteration 16 :  prob: [[ 0.18558762  0.11620964  0.30998959  0.19410657  0.19410657]]\n",
      "iteration 17 :  prob: [[ 0.18564148  0.11580929  0.31078871  0.19388026  0.19388026]]\n",
      "iteration 18 :  prob: [[ 0.18568418  0.11548688  0.31143407  0.19369744  0.19369744]]\n",
      "iteration 19 :  prob: [[ 0.18571817  0.11522711  0.31195526  0.19354973  0.19354973]]\n",
      "iteration 20 :  prob: [[ 0.18574533  0.11501771  0.31237617  0.19343039  0.19343039]]\n"
     ]
    }
   ],
   "source": [
    "# observed probabilities\n",
    "obs = np.array([[1.0],[0.3],[0.5]])\n",
    "\n",
    "# initial values are derived from the number of active features\n",
    "prob = np.array([active_feat / np.sum(active_feat)])\n",
    "print(\"       initial prob:\", prob)\n",
    "\n",
    "# lambda's\n",
    "lmbd = np.array([[0.0],[0.0],[0.0]])\n",
    "\n",
    "# the number of iterations\n",
    "num_iterations = 20\n",
    "\n",
    "# to record changes of probabilities for plotting\n",
    "record = np.zeros((num_iterations+1, num_feat))\n",
    "record[0,:] = prob\n",
    "\n",
    "# iterative scaling algorithm\n",
    "for i in range(num_iterations):\n",
    "    delta = np.exp(np.array([[0.0],[0.0],[0.0]]))\n",
    "\n",
    "    # newton-raphson\n",
    "    for j in range(10):\n",
    "        # numerator\n",
    "        a = np.array([delta[0]**active_feat, delta[1]**active_feat, delta[2]**active_feat]) * indicator\n",
    "        g = np.dot(a, prob.T) - obs\n",
    "\n",
    "        # denominator\n",
    "        b = active_feat - 1;\n",
    "        c = np.array([delta[0]**b, delta[1]**b, delta[2]**b]) * indicator\n",
    "        g_prime = np.dot(c, (prob * active_feat).T)\n",
    "\n",
    "        delta -= g / g_prime\n",
    "\n",
    "    # updating lambda's\n",
    "    lmbd += np.log(delta)\n",
    "    e = np.exp(np.dot(lmbd.T, indicator))\n",
    "    \n",
    "    # normalised by Z_{\\lambda}\n",
    "    prob = e / np.sum(e)\n",
    "    record[i+1,:] = prob\n",
    "    print(\"iteration\", i+1, \":  prob:\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical solution above can be compared with the analytical solution which we derived earlier:\n",
    "\\begin{align*}\n",
    "  p(misty) &= \\frac{9 - \\sqrt{51}}{10} = 0.186... \\\\\n",
    "  p(foggy) &= \\frac{\\sqrt{51} - 6}{10} = 0.114... \\\\\n",
    "  p(cloudy) &= \\frac{\\sqrt{51} - 4}{10} = 0.314... \\\\\n",
    "  p(sunny) &= \\frac{11 - \\sqrt{51}}{20} = 0.193... \\\\\n",
    "  p(rainy) &= \\frac{11 - \\sqrt{51}}{20} = 0.193...\n",
    "\\end{align*}\n",
    "We can also plot how the iterative scaling algorithm has reached the numerical solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAEZCAYAAAB/xedlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndP/Fx38AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25d073b5748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "\n",
    "plt.axis([-1,21,0,0.4])\n",
    "plt.title(\"weather forcast: numerical solution\")\n",
    "plt.xlabel(\"#iterations\")\n",
    "plt.ylabel(\"probability\")\n",
    "plt.plot(record[:,0], label=\"p(misty)\")\n",
    "plt.plot(record[:,1], label=\"p(foggy)\")\n",
    "plt.plot(record[:,2], label=\"p(cloudy)\")\n",
    "plt.plot(record[:,3], label=\"p(sunny)\")\n",
    "plt.plot(record[:,4], label=\"p(rainy)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.savefig('isa.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
