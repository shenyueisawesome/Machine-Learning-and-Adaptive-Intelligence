{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "- $k$-means and other clustering approaches\n",
    "- principal component analysis (PCA)\n",
    "- probabilistic PCA and latent variable models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import notebook as nb\n",
    "import mlai\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review and Preview\n",
    "\n",
    "* last week we saw Bayesian assigns a probability distribution on parameters\n",
    "* concept of conjugate priors was introduced\n",
    "* we saw Bayesian regularises the means\n",
    "* this week we look at unsupervised learning, in particular, $k$-means and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Learning\n",
    "\n",
    "Supervised learning is learning where each data has a label.\n",
    "\n",
    "- training example: set of pairs consisting of an input object and a desired output value\n",
    "- recall the bias-variance tradeoff (**week 5**)\n",
    "\n",
    "(eg) regression analysis\n",
    "\n",
    "Wikipedia: [Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "In unsupervised learning we have no labels for the data.\n",
    "It is often thought of as structure discovery, such as finding features in the data.\n",
    "\n",
    "Unsupervised learning techniques:\n",
    "- clustering (eg: k-means)\n",
    "- latent variable models (eg: EM algorithm)\n",
    "- blind signal separation (eg: PCA, ICA, SVD)\n",
    "\n",
    "Wikipedia: [Unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering\n",
    "\n",
    "Clustering is a class of important and widely used unsupervised learning techniques (but it is not covered very much in this module).\n",
    "\n",
    "It associate each data point $y_i$ with one of $k$ different discrete groups:\n",
    "- clustering animals into discrete groups (Are animals discrete or continuous?)\n",
    "- clustering into different different political affiliations\n",
    "\n",
    "Wikipedia: [Cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis)\n",
    "\n",
    "(next graph) clustering example of two dimensional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./diagrams/cluster_data00.svg'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_plots('cluster_data{counter:0>2}.svg', directory='./diagrams', counter=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Quantisation (VQ)\n",
    "\n",
    "VQ determines how to allocate each point to a group.\n",
    "- *harder* total number of groups\n",
    "- conceptually similar to clustering\n",
    "\n",
    "Wikipedia: [Vector quantization](https://en.wikipedia.org/wiki/Vector_quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $K$-means Clustering\n",
    "\n",
    "$k$-means is a simple algorithm for allocating $n$ data points into $K$ groups. \n",
    "It requires a set of $K$ cluster centres and the algorithm assigns each points to a cluster:\n",
    "1. initialize cluster centres as randomly selected data points\n",
    "2. assign each data point to the nearest cluster centre\n",
    "3. update each cluster centre by setting it to the mean of assigned data points\n",
    "4. repeat 2 and 3 until cluster allocations do not change\n",
    "\n",
    "Wikipedia: [K-means clustering](https://en.wikipedia.org/wiki/K-means_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $K$-means Objective Function\n",
    "\n",
    "$K$-means minimises the following objective function:\n",
    "$$\n",
    "  E = \\sum_{j=1}^K \\sum_{i\\ \\text{allocated to}\\ j} \\left( \\mathbf{y}_i - \\boldsymbol{\\mu}_j \\right)^\\top \\left(\\mathbf{y}_i - \\boldsymbol{\\mu}_j \\right)\n",
    "$$\n",
    "ie, it minimises the sum of Euclidean squared distances between data points $\\mathbf{y}_i$ and their associated centres $\\boldsymbol{\\mu}_j$.\n",
    "\n",
    "- this objective function is a non-convex optimisation problem\n",
    "- the minimum is NOT guaranteed to be global or unique\n",
    "\n",
    "(next graph) 4 iterations of $k$-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./diagrams/kmeans_clustering_002.svg'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_plots('kmeans_clustering_{counter:0>3}.svg', directory='./diagrams', text_top='kmeans_clustering_{counter:0>3}.tex', counter=(0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Clustering Approaches (I)\n",
    "\n",
    "**Spectral clustering** :\n",
    "- allowing clusters which aren't convex hulls\n",
    "\n",
    "Wikipedia: [Spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Clustering Approaches (II)\n",
    "\n",
    "**Dirichlet process** :\n",
    "- a probabilistic formulation for a clustering algorithm that is non-parametric\n",
    "- (loosely speaking) allowing infinite clusters\n",
    "- in practice useful for dealing with previously unknown species (eg. a **black swan event**)\n",
    "\n",
    "Wikipedia: [Dirichlet process](https://en.wikipedia.org/wiki/Dirichlet_process)\n",
    "\n",
    "Wikipedia: [Black swan theory](https://en.wikipedia.org/wiki/Black_swan_theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### High Dimensional Data\n",
    "\n",
    "USPS dataset for handwritten digits:\n",
    "- 3648 dimensions (64 rows, 57 columns)\n",
    "\n",
    "(next graph) space of 64 rows and 57 columns, including USPS sample '6'\n",
    "- space contains many more than just '6'\n",
    "- even if sampled at every nanonsecond from now until end of universe you won't see the original '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='./diagrams/dem_six001.png'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.display_plots('dem_six{counter:0>3}.png', directory='./diagrams', counter=(0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Low Dimensional Manifolds\n",
    "\n",
    "(observation)\n",
    "Digit '6' may be rotated, but pure rotation is too simple and, in practice, data may undergo several distortions.\n",
    "\n",
    "- for high dimensional data with structure, we expect fewer distortions than dimensions, ie, expect the data to live on a lower dimensional manifold\n",
    "\n",
    "(conclusion)\n",
    "High dimensional data may be dealt with by looking for a lower dimensional (non-)linear embedding.\n",
    "\n",
    "- $m$-dimensional data $\\mathbf{y}_i$ may be mapped to $d$-dimentional representation $\\mathbf{x}_i$, where $m>d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a linear embedding, which is presented as rotating the space to find **directions in data with maximal variance**.\n",
    "\n",
    "How do we find these directions?\n",
    "\n",
    "- diagonalising the sample covariance matrix $\\mathbf{S}$ :\n",
    "$$\n",
    "  \\mathbf{S} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{y}_i \\mathbf{y}_i^\\top\n",
    "$$\n",
    "for a set of $n$ data points $\\mathbf{y}_i$ with **zero mean**.\n",
    "\n",
    "- if $\\boldsymbol{\\mu} \\neq 0$, shift $\\mathbf{y}_i$ by $\\boldsymbol{\\mu}$ before calculating the covariance.\n",
    "\n",
    "Wikipedia: [Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Before PCA\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a6/PCA_linear_start.png\", width=500>\n",
    "\n",
    "- covariance has large values with non-diagonal elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### After PCA\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/63/PCA_linear_end.png\", width=500>\n",
    "\n",
    "- covariance is diagonal, or having very small values with non-diagonal elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA: Task Definition\n",
    "\n",
    "Here our view of an $m$-dimensional vector $\\mathbf{w}_1$ is a line through the origin.\n",
    "For simplicity we use $\\mathbf{w}_1$ having a unit length.\n",
    "The projection of a single, $m$-dimensional data point $\\mathbf{y}_i$ onto a line $\\mathbf{w}_1$ is given by\n",
    "$$\n",
    "  \\frac{\\mathbf{w}_1^\\top}{||\\mathbf{w}_1||} \\mathbf{y}_i = \\mathbf{w}_1^\\top \\mathbf{y}_i\n",
    "$$\n",
    "The covariance of the projected dataset is\n",
    "$$\n",
    "  \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{w}_1^\\top \\mathbf{y}_i) (\\mathbf{w}_1^\\top \\mathbf{y}_i)^\\top\n",
    "  = \\mathbf{w}_1^\\top \\left( \\frac{1}{n} \\sum_{i=1}^n \\mathbf{y}_i \\mathbf{y}_i^\\top \\right) \\mathbf{w}_1\n",
    "  = \\mathbf{w}_1^\\top \\mathbf{S} \\mathbf{w}_1\n",
    "$$\n",
    "Our task is to find directions for which the variance above is maximised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA: Lagrangian\n",
    "\n",
    "The solution is found by constrained optimisation using a Lagrange multiplier $\\lambda_1$ :\n",
    "$$\n",
    "  L(\\mathbf{w}_1,\\lambda_1) = \\mathbf{w}_1^\\top \\mathbf{S} \\mathbf{w}_1 + \\lambda_1 (1 - \\mathbf{w}_1^\\top \\mathbf{w}_1)\n",
    "$$\n",
    "The gradient with respect to $\\mathbf{w}_1$ is\n",
    "$$\n",
    "  \\nabla_{\\mathbf{w}_1} L = \\frac{\\partial L(\\mathbf{w}_1,\\lambda_1)}{\\partial \\mathbf{w}_1} = 2\\mathbf{S}\\mathbf{w}_1 - 2\\lambda_1 \\mathbf{w}_1\n",
    "$$\n",
    "We set $\\nabla_{\\mathbf{w}_1} L = 0$ and rearrange to form\n",
    "$$\n",
    "  \\mathbf{S} \\mathbf{w}_1 = \\lambda_1 \\mathbf{w}_1\n",
    "$$\n",
    "which is known as an **eigenvalue problem**.\n",
    "\n",
    "Wikipedia:\n",
    "[Lagrange multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier) /\n",
    "[Eigenvalues and eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA: Projection of a Single Data Point\n",
    "\n",
    "- the optimised solution $\\mathbf{S} \\mathbf{w}_1 = \\lambda \\mathbf{w}_1$ indicates that the desired direction $\\mathbf{w}_1$ is an eigenvector of $\\mathbf{S}$\n",
    "\n",
    "- further directions $\\mathbf{w}_2,\\mathbf{w}_3,\\ldots$ that are **orthogonal** to $\\mathbf{w}_1$ can also be shown to be eigenvectors of the covariance $\\mathbf{S}$\n",
    "\n",
    "Arranging eigenvectors to create an $m\\times d$ matrix:\n",
    "$$\n",
    "  \\mathbf{W}\n",
    "  = (\\mathbf{w}_1, \\ldots, \\mathbf{w}_d)\n",
    "  = \\left(\\begin{array}{ccc} w_{11} & \\cdots & w_{1d} \\\\ \\vdots & & \\vdots \\\\ w_{m1} & \\cdots & w_{md} \\end{array}\\right)\n",
    "$$\n",
    "and $d$-dimensional projection $\\mathbf{x}_i$ of an $m$-dimentional data point $\\mathbf{y}_i$ is\n",
    "$$\n",
    "  \\mathbf{x}_i = \\mathbf{W}^\\top \\mathbf{y}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA: Projection of a Dataset\n",
    "\n",
    "Projection $\\mathbf{X}$ of a dataset $\\mathbf{Y}$, consisting of $n$ data points, is given by\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{Y}\\mathbf{W}\n",
    "$$\n",
    "- $m\\times d$ matrix $\\mathbf{W}$ of eigenvectors is as before\n",
    "- $n\\times d$ projection $\\mathbf{X}$ and $n\\times m$ dataset $\\mathbf{Y}$ are given by\n",
    "$$\n",
    "  \\mathbf{X}\n",
    "  = \\left(\\begin{array}{c} \\mathbf{x}_1^\\top \\\\ \\vdots \\\\ \\mathbf{x}_n^\\top \\end{array}\\right)\n",
    "  = \\left(\\begin{array}{ccc} x_{11} & \\cdots & x_{1d} \\\\ \\vdots & & \\vdots \\\\ x_{n1} & \\cdots & x_{nd} \\end{array}\\right)\n",
    "  \\qquad\n",
    "  \\mathbf{Y}\n",
    "  = \\left(\\begin{array}{c} \\mathbf{y}_1^\\top \\\\ \\vdots \\\\ \\mathbf{y}_n^\\top \\end{array}\\right)\n",
    "  = \\left(\\begin{array}{ccc} y_{11} & \\cdots & y_{1m} \\\\ \\vdots & & \\vdots \\\\ y_{n1} & \\cdots & y_{nm} \\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA:  Eigenvalue Decomposition\n",
    "\n",
    "In practice eigenvectors can be derived by **eigenvalue decomposition** of a covariance.\n",
    "\n",
    "- $n\\times n$ covariance matrix $\\mathbf{S}$ can be factorised as\n",
    "$$\n",
    "  \\mathbf{S} = \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{V}^{-1}\n",
    "$$\n",
    "where $\\mathbf{V}$ is $n\\times n$ matrix whose columns are the eigenvectors of $\\mathbf{S}$ and $\\boldsymbol{\\Lambda}$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues\n",
    "\n",
    "Alternative is to use **singular value decomposition** (**SVD**).\n",
    "\n",
    "Wikipedia:\n",
    "[Eigendecomposition of a matrix](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) /\n",
    "[SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA: Recap\n",
    "\n",
    "Let $\\lambda_1, \\lambda_2, \\ldots$ be eigenvalues of $\\mathbf{S}$ in non-increasing order, and $\\mathbf{w}_1, \\mathbf{w}_2, \\ldots$ be corresponding eigenvectors:\n",
    "\n",
    "- PCA changes the basis: projection basis is a **linear combination** of measurement basis\n",
    "- $\\mathbf{w}_1, \\mathbf{w}_2, \\ldots$ are principal components that are **orthogonal**\n",
    "- $\\mathbf{w}_1$ is the direction preserving the largest variance, and the resulting variance is simply $\\lambda_1$\n",
    "- $\\mathbf{w}_i$ is the direction preserving the $i^{th}$ largest variance\n",
    "- large variances pertain the important structure of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Latent Variables\n",
    "\n",
    "Limitation of PCA:\n",
    "- no probabilistic model for observed data\n",
    "- computation intensive for large dataset with high dimension\n",
    "- missing data problem\n",
    "- affected by outliers\n",
    "\n",
    "In the following we introduce **latent variables** (hidden, or unobserved variables) to represent data and their dependencies in lower dimension.\n",
    "\n",
    "Wikipedia: [Latent variable](https://en.wikipedia.org/wiki/Latent_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Latent Variable Model\n",
    "\n",
    "There are $n$ data points: $\\mathbf{y}_i$ that are observable in the $m$-dimensional space.\n",
    "We would like to represent each $\\mathbf{y}_i$ using a latent variable $\\mathbf{x}_i$ in the $d$-dimensional space.\n",
    "Note that $d<m$.\n",
    "\n",
    "We assume that latent variables relate to observed data points through principal axes $\\mathbf{W}$, which is an $m\\times d$ matrix.\n",
    "This leads to a linear relationship of the form:\n",
    "$$\n",
    "  \\mathbf{y}_i = \\mathbf{W} \\mathbf{x}_i + \\boldsymbol{\\epsilon}_i\n",
    "$$\n",
    "where $\\boldsymbol{\\epsilon}_i \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I})$ with a noise term $\\sigma^2$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic PCA\n",
    "\n",
    "We assume that each latent variable is normally distributed:\n",
    "$$\n",
    "  \\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})\n",
    "$$\n",
    "\n",
    "The corresponding data point $\\mathbf{y}_i$ is generated by projection:\n",
    "$$\n",
    "  \\mathbf{y}_i | \\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{W}\\mathbf{x}_i,\\sigma^2\\mathbf{I})\n",
    "$$\n",
    "\n",
    "**Probabilistic PCA** aims to estimate the principal axes $\\mathbf{W}$ and the noise $\\sigma^2$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic PCA\n",
    "\n",
    "Firstly, $\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$ implies that\n",
    "$$\n",
    "  \\mathbf{W} \\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{W}\\mathbf{W}^\\top)\n",
    "$$\n",
    "\n",
    "Further, given assumption $\\boldsymbol{\\epsilon}_i \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I})$ with a linear relation $\\mathbf{y}_i = \\mathbf{W} \\mathbf{x}_i + \\boldsymbol{\\epsilon}_i$,\n",
    "$$\n",
    "  \\mathbf{y}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{W}\\mathbf{W}^\\top + \\sigma^2\\mathbf{I})\n",
    "$$\n",
    "\n",
    "**Classical PCA** is the special case of **probabilistic PCA** when the covariance of the noise appraoches zero, ie, $\\sigma^2 \\rightarrow 0$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic PCA\n",
    "\n",
    "The standard approach to solution using latent variable models is\n",
    "1. difine a Gaussian prior over latent space $\\mathbf{X}$\n",
    "2. integrate out latent variables\n",
    "\n",
    "In probabilistic PCA marginal likelihood for data matrix $\\mathbf{Y}$ has the form:\n",
    "$$\n",
    "  p(\\mathbf{Y} | \\mathbf{W},\\sigma^2) = \\prod_{i=1}^n \\mathcal{N}(\\mathbf{y}_i | \\mathbf{0}, \\mathbf{W}\\mathbf{W}^\\top + \\sigma^2\\mathbf{I})\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
