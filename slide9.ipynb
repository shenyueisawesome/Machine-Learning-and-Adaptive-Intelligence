{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### week 9\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "- classification\n",
    "- Bernoulli distribution\n",
    "- naive Bayes classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import notebook as nb\n",
    "import mlai\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review and Preview\n",
    "\n",
    "- last week we Looked at the EM algorithm and its application to solve a maximum entropy model\n",
    "- approaches to an analytical solution and a numerical solution are introduced\n",
    "- this week we go back to the main road\n",
    "- the material concerns classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification\n",
    "\n",
    "We are given a dataset containing 'inputs' $\\mathbf{X}$ and the corresponding 'target values' $\\mathbf{y}$.\n",
    "- each point consists of an input vector $\\mathbf{x}_i$ and a class label $y_i$\n",
    "- $\\mathbf{x}_i$ can be thought of as features\n",
    "\n",
    "**Bernoulli trial** : random experiments with binary classification\n",
    "- $y_i$ is either $1$ (yes) or $0$ (no)\n",
    "\n",
    "Wikipedia: [Bernoulli trial](https://en.wikipedia.org/wiki/Bernoulli_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Examples\n",
    "\n",
    "- classification of hand written digits from binary images (eg, automatic postcode reading)\n",
    "- face detection in images (eg, digital cameras)\n",
    "- who a detected face belongs to (eg, Picasa, Facebook, DeepFace, GaussianFace)\n",
    "- classification of cancer type given gene expression data\n",
    "- categorisation of document types (eg, different types of news article on the internet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reminder on the Term 'Bayesian'\n",
    "\n",
    "We use Bayes' rule to invert probabilities in the Bayesian approach.\n",
    "\n",
    "Bayesian is not named after Bayes' rule (common confusion).\n",
    "The use of Bayes' rule does **not** imply you are being Bayesian.\n",
    "It is just an application of the product rule of probability.\n",
    "\n",
    "- the term 'Bayesian' refers to the treatment of the parameters as stochastic variables\n",
    "- proposed by Laplace and Bayes independently\n",
    "- very controversial for early statisticians (eg, Fisher et al)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discrete Probability Distribution\n",
    "\n",
    "Discrete probability is characterised by a **probability mass function**.\n",
    "\n",
    "(eg) Poisson distribution, Bernoulli distribution, binomial distribution\n",
    "\n",
    "- **regression** : a prediction function $f(\\mathbf{X})$ may be a real number or sometimes real vector\n",
    "- **classification** : we are given an input vector $\\mathbf{x}$ and the associated label $y$ which takes value $0$ or $1$\n",
    "\n",
    "Wikipedia: [Probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bernoulli Distribution\n",
    "\n",
    "The **Bernoulli distribution** represents any single experiment that asks a yesâ€“no question.\n",
    "\n",
    "Its probabbility mass function is given by\n",
    "$$\n",
    "  p(y) = \\left\\{ \\begin{array}{ll} \\pi & y = 0 \\\\ 1-\\pi & y = 1 \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "(eg) $\\pi = 0.5$ for coin toss with a fair coin\n",
    "\n",
    "Wikipedia: [Bernoulli distribution](http://en.wikipedia.org/wiki/Bernoulli_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bernoulli Distribution\n",
    "\n",
    "Mathematically we use $y$ as a **mathematical switch**:\n",
    "$$\n",
    "  P(y) = \\pi^y (1-\\pi)^{(1-y)}\n",
    "$$\n",
    "\n",
    "It is a clever trick for switching probabilities, as code it would be\n",
    "```python\n",
    "def bernoulli(y_i, pi):\n",
    "    if y_i == 1:\n",
    "        return pi\n",
    "    else:\n",
    "        return 1-pi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jacob Bernoulli's Bernoulli\n",
    "\n",
    "Bernoulli described the Bernoulli distribution in terms of an 'urn' filled with balls.\n",
    "\n",
    "- there are red and black balls, and there is a fixed number of balls in the urn\n",
    "- the portion of red balls is given by $\\pi$\n",
    "- there is considered to be **epistemic uncertainty** about the distribution parameter by Bernoulli\n",
    "\n",
    "Wikipedia: [Jacob Bernoulli](https://en.wikipedia.org/wiki/Jacob_Bernoulli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thomas Bayes's Bernoulli\n",
    "\n",
    "Bayes described the Bernoulli distribution (he didn't call it that!) in terms of a table and two balls.\n",
    "\n",
    "- each ball is rolled so it comes to rest at a position that is uniformly distributed across the table\n",
    "- the first ball stops at a position that is $\\pi$ times the table width\n",
    "- after placing the first ball you consider whether the second would land to the left or the right\n",
    "- there is **aleatoric uncertainty** about Bayes's distribution\n",
    "\n",
    "Wikipedia: [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Epistemic and Aleatoric Uncertainties\n",
    "\n",
    "Epistemic uncertainty:\n",
    "- this is uncertainty we could in principal know the answer; we just have not observed enough yet\n",
    "- (eg) the result of a football match **after** it's played\n",
    "\n",
    "Aleatoric Uncertainty:\n",
    "- this is uncertainty we could not know even if we wanted to\n",
    "- (eg) the result of a football match **before** it is played"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood in the Bernoulli (I)\n",
    "\n",
    "We assume that\n",
    "- data $\\mathbf{y}$ is a binary sequence of length $n$\n",
    "- each value was sampled independently from the Bernoulli distribution, given probability $\\pi$\n",
    "\n",
    "The likelihood for the dataset $\\mathbf{y}$ is given by\n",
    "$$\n",
    "  p(\\mathbf{y}|\\pi) = \\prod_{i=1}^n p(y_i | \\pi) = \\prod_{i=1}^n \\pi^{y_i} (1-\\pi)^{1-y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood in the Bernoulli (II)\n",
    "\n",
    "The objective function is the negative log likelihood:\n",
    "$$\n",
    "  E(\\pi) \\buildrel\\triangle\\over = -\\log p(\\mathbf{y} | \\pi) = -\\sum_{i=1}^n y_i \\log \\pi - \\sum_{i=1}^n (1-y_i) \\log(1-\\pi)\n",
    "$$\n",
    "Gradient with respect to the parameter $\\pi$:\n",
    "$$\n",
    "  \\frac{\\partial E(\\pi)}{\\partial\\pi} = -\\frac{\\sum_{i=1}^n y_i}{\\pi} + \\frac{\\sum_{i=1}^n (1-y_i)}{1-\\pi}\n",
    "$$\n",
    "At a stationary point we set $\\frac{\\partial E(\\pi)}{\\partial\\pi} = 0$ and get\n",
    "$$\n",
    "  \\pi = \\frac{1}{n}\\sum_{i=1}^n y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood in the Bernoulli (III)\n",
    "\n",
    "The solution:\n",
    "$$\n",
    "  \\pi = \\frac{1}{n}\\sum_{i=1}^n y_i\n",
    "$$\n",
    "makes intuitive sense, ie, we estimate the probability associated with the Bernoulli by setting it to the number of observed positives, divided by the total number of experiments.\n",
    "\n",
    "(eg) when you get 47 heads from 100 tosses, what is your best guess of probability for heads from this coin toss experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes' Rule Reminder\n",
    "\n",
    "Bayes' rule consists of four components; likelihood, marginal likelihood, prior and posterior distributions in the following relation:\n",
    "$$\n",
    "  \\text{posterior} = \\frac{\\text{likelihood}\\times\\text{prior}}{\\text{marginal likelihood}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Classifiers\n",
    "\n",
    "Concept for naive Bayes classifers:\n",
    "\n",
    "- reduce the number of parameters we need to optimise by computing the distribution using the product and sum rules\n",
    "\n",
    "- given inputs $\\mathbf{X}$ and the label data $\\mathbf{y}$, specify a joint density $p(\\mathbf{y}, \\mathbf{X})$ for all potential values of $\\mathbf{X}$ and $\\mathbf{y}$\n",
    "\n",
    "- given a test input $\\mathbf{x}^*$, extend the density as $p(y^* | \\mathbf{X}, \\mathbf{y}, \\mathbf{x}^*)$ to calculate the test label $y^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Assumptions\n",
    "\n",
    "In **naive Bayes** we make certain simplifying assumptions that allow us to perform all of the above in practice.\n",
    "- data conditional independence\n",
    "- feature conditional independence\n",
    "- marginal density for $y_i$\n",
    "\n",
    "They are very strong (naive) assumptions about factorisations that are unlikely to be true in practice.\n",
    "\n",
    "Wikipedia: [Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(note) **conditional independence**\n",
    "\n",
    "Two events A and B are **conditionally independent** given a third event C if the occurrence of A and the occurrence of B are independent events in their conditional probability distribution given C.\n",
    "\n",
    "- A and B are **conditionally independent** given C if and only if, given knowledge that C occurs, knowledge of whether A (or B) occurs provides no information on the likelihood of B (or A) occurring\n",
    "\n",
    "Wikipedia: [Conditional independence](https://en.wikipedia.org/wiki/Conditional_independence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Conditional Independence\n",
    "\n",
    "Given model parameters $\\boldsymbol{\\theta}$ we assume that all data points in the model are **conditionally independent**:\n",
    "$$\n",
    "  p(\\mathbf{x}^*, y^*, \\mathbf{X}, \\mathbf{y} | \\boldsymbol{\\theta}) = p(\\mathbf{x}^*, y^* | \\boldsymbol{\\theta}) \\prod_{i=1}^n p(\\mathbf{x}_i, y_i | \\boldsymbol{\\theta})\n",
    "$$\n",
    "- joint density of $\\mathbf{X}$ and $\\mathbf{y}$ is independent across the data given $\\boldsymbol{\\theta}$\n",
    "- similar assumption for **regression**, where $\\boldsymbol{\\theta} = \\{\\mathbf{w},\\sigma^2\\}$\n",
    "\n",
    "Computing posterior distribution in this case becomes easier, which is known as the **Bayes classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Conditional Independence\n",
    "\n",
    "We now assume that features are also **conditionally independent** given parameters and the class label:\n",
    "$$\n",
    "  p(\\mathbf{x}_i | y_i, \\boldsymbol{\\theta}) = \\prod_{j=1}^p p(x_{i,j} | y_i, \\boldsymbol{\\theta})\n",
    "$$\n",
    "where $p$ is the dimensionality of the inputs.\n",
    "\n",
    "- very strong assumption: particular to naive Bayes\n",
    "- known as the **naive Bayes assumption**\n",
    "- Bayes classifier + feature conditional independence = naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Marginal Density for $y_i$\n",
    "\n",
    "To calculate the joint distribution we need the marginal for $p(y_i)$ :\n",
    "$$\n",
    "  p(x_{i,j}, y_i | \\boldsymbol{\\theta}) = p(x_{i,j} | y_i, \\boldsymbol{\\theta}) p(y_i)\n",
    "$$\n",
    "Because $y_i$ is binary, the **Bernoulli density** makes a suitable choice for our prior over $y_i$ :\n",
    "$$\n",
    "  p(y_i | \\pi) = \\pi^{y_i} (1-\\pi)^{1-y_i}\n",
    "$$\n",
    "where $\\pi$ now has the interpretation as being the **prior** probability that the classification should be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood for Naive Bayes (I)\n",
    "\n",
    "The full joint density of the training data is\n",
    "$$\n",
    "  p(\\mathbf{X}, \\mathbf{y} | \\boldsymbol{\\theta}, \\pi) = \\prod_{i=1}^n \\prod_{j=1}^p p(x_{i,j} | y_i, \\boldsymbol{\\theta}) p(y_i | \\pi)\n",
    "$$\n",
    "The objective function is the negative log likelihood:\n",
    "$$\n",
    "  E(\\boldsymbol{\\theta}, \\pi) \\buildrel\\triangle\\over = -\\log p(\\mathbf{X}, \\mathbf{y} | \\boldsymbol{\\theta}, \\pi) = -\\sum_{i=1}^n \\sum_{j=1}^p \\log p(x_{i,j} | y_i, \\boldsymbol{\\theta}) - \\sum_{i=1}^n \\log p(y_i | \\pi)\n",
    "$$\n",
    "which we decompose into two objective functions, one which is dependent on $\\pi$ alone and one which is dependent on $\\boldsymbol{\\theta}$ alone:\n",
    "$$\n",
    "  E(\\boldsymbol{\\theta}, \\pi) = E(\\boldsymbol{\\theta}) + E(\\pi)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood for Naive Bayes (II)\n",
    "\n",
    "To minimise the prior:\n",
    "$$\n",
    "  E(\\pi) = -\\sum_{i=1}^n \\log p(y_i | \\pi)\n",
    "$$\n",
    "is identical to the objective function from the Bernoulli; so we have\n",
    "$$\n",
    "  \\pi = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood for Naive Bayes (III)\n",
    "\n",
    "To minimise the conditional distribution:\n",
    "$$\n",
    "  E(\\boldsymbol{\\theta}) = -\\sum_{i=1}^n \\sum_{j=1}^p \\log p(x_{i,j} | y_i, \\boldsymbol{\\theta})\n",
    "$$\n",
    "we make an assumption about its form.\n",
    "\n",
    "- the right assumption will depend on the data\n",
    "- (eg) we may use Gaussian for real values data:\n",
    "$$\n",
    "  p(x_{i,j} | y_i, \\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y_i,j}^2}} \\exp\\left( -\\frac{(x_{i,j} - \\mu_{y_i,j})^2}{\\sigma_{y_i,j}^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making Predictions (I)\n",
    "\n",
    "Finally we look at how to evluate $p(y^* | \\mathbf{X}, \\mathbf{y}, \\mathbf{x}^*, \\boldsymbol{\\theta})$ .\n",
    "\n",
    "Using the product rule:\n",
    "$$\n",
    "  p(y^* | \\mathbf{X}, \\mathbf{y}, \\mathbf{x}^*, \\boldsymbol{\\theta}) p(\\mathbf{X}, \\mathbf{y}, \\mathbf{x}^* | \\boldsymbol{\\theta})\n",
    "  = p(\\mathbf{X}, \\mathbf{y}, \\mathbf{x}^*, y^* | \\boldsymbol{\\theta})\n",
    "$$\n",
    "This implies\n",
    "$$\n",
    "  p(y^* | \\mathbf{X}, \\mathbf{y}, \\mathbf{x}^*, \\boldsymbol{\\theta})\n",
    "  = \\frac{p(\\mathbf{X}, \\mathbf{y}, \\mathbf{x}^*, y^* | \\boldsymbol{\\theta})}{p(\\mathbf{X}, \\mathbf{y}, \\mathbf{x}^* | \\boldsymbol{\\theta})}\n",
    "  = \\frac{p(\\mathbf{X}, \\mathbf{y}, \\mathbf{x}^*, y^* | \\boldsymbol{\\theta})}{\\sum_{y^*=0,1} p(\\mathbf{X}, \\mathbf{y}, \\mathbf{x}^*, y^* | \\boldsymbol{\\theta})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making Predictions (II)\n",
    "\n",
    "Here we use **data conditional independence** :\n",
    "$$\n",
    "  p(\\mathbf{x}^*, y^*, \\mathbf{X}, \\mathbf{y} | \\boldsymbol{\\theta})\n",
    "  = p(\\mathbf{x}^*, y^* | \\boldsymbol{\\theta}) \\prod_{i=1}^n p(\\mathbf{x}_i, y_i | \\boldsymbol{\\theta})\n",
    "$$\n",
    "which gives a solution for making predictions:\n",
    "$$\n",
    "  p(y^* | \\mathbf{X}, \\mathbf{y}, \\mathbf{x}^*, \\boldsymbol{\\theta})\n",
    "  = \\frac{p(\\mathbf{x}^*, y^* | \\boldsymbol{\\theta}) \\prod_{i=1}^n p(\\mathbf{x}_i, y_i | \\boldsymbol{\\theta})}{\\sum_{y^*=0,1} p(\\mathbf{x}^*, y^* | \\boldsymbol{\\theta}) \\prod_{i=1}^n p(\\mathbf{x}_i, y_i | \\boldsymbol{\\theta})}\n",
    "  = \\frac{p(\\mathbf{x}^*, y^* | \\boldsymbol{\\theta})}{\\sum_{y^*=0,1} p(\\mathbf{x}^*, y^* | \\boldsymbol{\\theta})}\n",
    "  = \\frac{\\prod_{j=1}^p p(x_j^* | y^*, \\boldsymbol{\\theta}) p(y^* | \\pi)}{\\sum_{y^*=0,1} \\prod_{j=1}^p p(x_j^* | y^*, \\boldsymbol{\\theta}) p(y^* | \\pi)}\n",
    "$$\n",
    "\n",
    "- recall **Bernoulli density** for calculating $p(y^* | \\pi)$\n",
    "- naive Bayes has derived the class conditional densities, $p(\\mathbf{x}_i | y_i, \\boldsymbol{\\theta})$, which is then used to evaluate $p(x_j^* | y^*, \\boldsymbol{\\theta})$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
